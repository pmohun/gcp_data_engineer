{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCP Professional Data Engineer\n",
    "### Leveraging Unstructured Data with Cloud Dataproc on Google Cloud Platform\n",
    "#### Modules:\n",
    "- Introduction to Cloud Dataproc\n",
    "- Running Dataproc Jobs\n",
    "- Leveraging GCP\n",
    "- Analyzing Unstructured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 1: Introduction to Cloud Dataproc\n",
    "### Introducing Cloud Dataproc\n",
    "- Cloud Dataproc is a fast, easy-to-use, fully-managed cloud service for running Apache Spark and Apache Hadoop clusters in a simpler, more cost-efficient way. Operations that used to take hours or days take seconds or minutes instead, and you pay only for the resources you use (with per-second billing). Cloud Dataproc also easily integrates with other Google Cloud Platform (GCP) services, giving you a powerful and complete platform for data processing, analytics and machine learning.\n",
    "\n",
    "### Why Unstructured Data\n",
    "- About 90% of enterprise data that is collected by a business tends to be unstructured. This includes:\n",
    "    - Emails, reviews, text, etc.\n",
    "- Consider the Google Steetview initiative- cars gathering photos at the street level with no immediate impact, yet now is the foundation for one of the most useful datasets available for autonomous cars.\n",
    "    \n",
    "### Why Cloud Dataproc\n",
    "- Horizontal vs vertical scaling\n",
    "- Hadoop origins:\n",
    "    - Based on a whitepaper describing MapReduce, Hadoop is an open-source distributed file system also known as HDFS. Spark is a framework that takes advantage of the distributed file system to effectively process tasks.\n",
    "- Running an onsite Hadoop cluster is costly and often inefficient.\n",
    "- Additional benefits:\n",
    "    - Stateless clusters in <90 seconds\n",
    "    - Supports Hadoop, Spark, Pig, Hive\n",
    "    - High-level APIs for job submission\n",
    "    - Connectors in BigTable,  BigQuery, Cloud Storage\n",
    "\n",
    "### Lab: Create a Dataproc Cluster\n",
    "\n",
    "#### Objectives:\n",
    "- Prepare a bucket for cluster initialization\n",
    "- Create a Dataproc Hadoop Cluster customized to use the Google Cloud API\n",
    "- Enable secure access to the Dataproc cluster\n",
    "- Explore Hadoop operations\n",
    "\n",
    "#### Task 1: Prepare Environment Variables\n",
    "- In the Console, on the Navigation menu () click Compute Engine > VM instances.\n",
    "- Locate the line with the instance called training_vm.\n",
    "- On the far right, under 'connect', Click on SSH to open a terminal window.\n",
    "- In this lab you will enter CLI commands on the training_vm.\n",
    "##### Create the source file for setting and resetting environment variables\n",
    "\n",
    "\n",
    "- In the training_vm SSH terminal window, using your preferred command line editor, create and edit the file to hold your environment variables. For example:\n",
    "- One environment variable that you will set is 'PROJECT_ID' that contains the Google Cloud project ID required to access billable resources.\n",
    "- In the Console, on the Navigation menu () click Home. In the panel with Project Info, the Project ID is listed. You can also find this information in the Qwiklabs tab under Connection Details, where it is labeled GCP Project ID.\n",
    "Add the environment variable to myenv for easy reference.\n",
    "- Dataproc can use a Cloud Storage bucket to stage its files during initialization. You can use this bucket to stage application programs or data for use by Dataproc. The bucket can also host Dataproc initialization scripts and output. The bucket name must be globally unique. Qwiklabs has already created a bucket for you that has the same name as the Project ID, which is already globally unique.\n",
    "- In the Console, on the Navigation menu () click Storage > Browser. Verify that the bucket exists. Notice the default storage class and the location (region) of this bucket. You will be using this region information next.\n",
    "Add the line to myenv to create an environment variable named \"BUCKET\".\n",
    "- You can use BUCKET in CLI commands. And if you need to enter the bucket name <your-bucket> in a text field in Console, you can quickly retrieve the name with echo BUCKET.\n",
    "- You will be creating a Dataproc cluster in a specific region. The Dataproc cluster and the bucket it will use for staging must be in the same region. Since the bucket you are using already exists, you will need to match the environment variable $MYREGION to the bucket region.\n",
    "- You can use find the region used by Qwiklabs on the Qwiklabs tab under Connection Details, labeled QL Region.\n",
    "The zone must be in the same region MYZONE will contain this value.\n",
    "- You can find the zone used by Qwiklabs on the Qwiklabs tab under Connection Details, labeled QL Zone.\n",
    "- Add the environment variables to myenv for easy reference.\n",
    "You will use the browser IP address to enable your local browser to reach the Dataproc cluster.\n",
    "- Find your computer's browser IP address by opening a browser window and viewing http://ip4.me/ Copy the IP address.\n",
    "Add the line to myenv to create an environment variable named BROWSER_IP.\n",
    "- After you have added all three definitions to myenv, and saved the file, use the source command to create the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-959bb4230150>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-959bb4230150>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    cd ~\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "cd ~ \n",
    "nano myenv\n",
    "PROJECT_ID=<project ID>\n",
    "BUCKET=<project ID>\n",
    "MYREGION=<region>\n",
    "MYZONE=<zone>\n",
    "BROWSER_IP=<your-browser-ip>\n",
    "\n",
    "# set environment variables\n",
    "source myenv\n",
    "# verify variables are set\n",
    "echo $PROJECT_ID\n",
    "echo $MYREGION $MYZONE\n",
    "echo $BUCKET\n",
    "echo $BROWSER_IP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2. Create a Dataproc Cluster\n",
    "- In the Console, on the Navigation menu () click Dataproc > Clusters.\n",
    "- Click Create Cluster.\n",
    "- Specify the following, and leave the remaining settings as their defaults:\n",
    "- Click on Preemptible workers, bucket, network, version, initialization, & access options\n",
    "- Specify the following, and leave the remaining settings as their defaults:\n",
    "    - Name: Cluster Dataproc\n",
    "    - Region: <myregion>\n",
    "    - Zone: <myzone>\n",
    "    - Cluster mode: Standard (1 master, n workers)\n",
    "    - (Master node) Machine type: n1-standard-2\n",
    "    - (Master node) Primary disk size: 100GB\n",
    "    - (Worker node) Machine type: n1-standard-1\n",
    "    - (Worker node) Primary disk size: 50GB\n",
    "    - Nodes: 3\n",
    "    - Network tags: hadoop access\n",
    "    - Cloud storage staging bucket: <your bucket>\n",
    "    - Image version: 1.2\n",
    "    - Project access: Allow API access\n",
    "    \n",
    "    \n",
    "- Create.\n",
    "- The cluster will take several minutes to become operational. In the Console, on the Navigation menu () click Dataproc > Clusters.\n",
    "- Click on your cluster, cluster-dataproc. Then click on the VM Instances tab. The instances will become operational before the hadoop software has completed initialization. When a checkmark in a green circle appears next to the name of the cluster, it is operational.\n",
    "\n",
    "#### Task 3: Enable secure access to Dataproc cluster\n",
    "- Create a firewall rule that allows access only to the Master Node from your computer's IP address. Only ports 8088 (Hadoop Job Interface) and 9870 (Hadoop Admin interface) will be permitted.\n",
    "- Port 8042 is the web UI for the node manager on the worker nodes and port 8080 is the default port for Datalab. Datalab is a notebook-based integrated development environment derived from Jupyter notebooks. It is a common tool for developing Dataproc applications. The Serverless Machine Learning on GCP course uses Datalab extensively.\n",
    "- Recall your computer's browser IP address for use in Console.\n",
    "\n",
    "    ```echo $BROWSER_IP```\n",
    "\n",
    "\n",
    "- In the Console, on the Navigation menu () click VPC Network > Firewall rules.\n",
    "- Click Create Firewall Rule.\n",
    "- Specify the following, and leave the remaining settings as their defaults:\n",
    "    - Name: allow-hadoop\n",
    "    - Network: default\n",
    "    - Priority: 1000\n",
    "    - Direction of traffic: Ingress\n",
    "    - Action on match: allow\n",
    "    - Targets: specified target tags\n",
    "    - Target tags: hadoopaccess\n",
    "    - Source IP ranges: <yourIP>32\n",
    "    - Specified ports and protocols tcp:9870;tcp:8088\n",
    "    \n",
    "- Verify that the network tag \"hadoopaccess\" is set on the Master Node. That will apply the firewall rule to the Master Node, giving your laptop access to it.\n",
    "- In the Console, on the Navigation menu () click Compute Engine > VM Instances.\n",
    "- Click on the Master Node, cluster-dataproc-m.\n",
    "- Verify that under Network Tags it lists hadoopaccess.\n",
    "- If the tag is not there, Click EDIT.\n",
    "- Under Network Tags add the tag: hadoopaccess\n",
    "- Click Save.\n",
    "\n",
    "#### Task 4: Explore Hadoop Operations\n",
    "- In the Console, on the Navigation menu () click Compute Engine > VM Instances.\n",
    "- In the list of VM instances, in the row for cluster-dataproc-m, highlight the External IP and copy it.\n",
    "- Open a new browser tab or window and paste the External IP. Add \":8088\" after the IP and press enter. Example: <External IP>:8088 The web page displayed is the Hadoop Applications interface.\n",
    "- Open a new browser tab or window. Paste the External IP. Add \":9870\" after the IP and press return. Example: <External IP>:9870 The webpage displayed is the Hadoop Administration Interface and should look something like this:\n",
    "- Click on the Datanodes tab. This will show you how much capacity is being used on the worker nodes HDFS (Hadoop Distributed File System) and how much capacity remains.\n",
    "- Click on Utilities > Logs. This shows you the Hadoop log files for each node in the cluster. This is where you can go to investigate problems with Hadoop. Use your browser's back button to return to the Hadoop Administration console.\n",
    "- Click on Utilities > Browse the file system. After a few moments the file system will appear in the browser page. You can use this to navigate the files system. In the row that says Owner is hdfs and Group is hadoop, click on the link that says user. -- - Here you can see directories for all the hadoop services.\n",
    "- Leave the JobTracker <External IP>:8088 and the Administration Interface <External IP>:9870 tabs or windows open. You will use them in the next task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 1 Review\n",
    "\n",
    "1.) Which of the following statements is true about Cloud Dataproc?\n",
    "- Lets you run Spark and Hadoop clusters with minimal administrations\n",
    "- Helps you create job-specific clusters w/o HDFS\n",
    "\n",
    "2.) Matching definitions:\n",
    "- Zone: determines the Google data center where compute nodes will be\n",
    "- Preemtible: costs less but may not always be available\n",
    "- Standard cluster mode: Provides 1 master and n workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 2: Running Dataproc Jobs\n",
    "### Running Jobs\n",
    "### Lab: Work with structured and semi-structured Data\n",
    "### Separation of Storage & Compute\n",
    "### Submitting Jobs\n",
    "### Spark RDDs, Transformations, and Actions\n",
    "### Lab: Working with Spark Jobs\n",
    "### Module 2 Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 3: Leveraging GCP\n",
    "### Big Query Support\n",
    "### Lab: Leverage GCP\n",
    "### Customizing Clusters\n",
    "### Lab: Cluster Automation using CLI Commands\n",
    "### Module 3 Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4: Analyzing Unstructured Data\n",
    "### Infuse Your Business With Machine Learning\n",
    "### Lab: Add Machine Learning\n",
    "### Module 4 Review"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
