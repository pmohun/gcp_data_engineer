{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCP Professional Data Engineer\n",
    "### Serverless Data Analysis with Google BigQuery and Cloud Dataflow\n",
    "#### Modules:\n",
    "- Serverless Data Analysis with BigQuery\n",
    "- Autoscaling Data Processing Pipelines with Dataflow\n",
    "\n",
    "#### Learning Objectives:\n",
    "\n",
    "- Build up a complex BigQuery using clauses, inner selects, built-in functions and joins\n",
    "- Load and export data to/from BigQuery\n",
    "- Identify need for nested, repeated fields and user-defined functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 1: Serverless Data Analysis with Big Query\n",
    "#### Topics:\n",
    "- Queries\n",
    "- Functions\n",
    "- Load & export data\n",
    "- Nested, repeated fields\n",
    "- Window functions\n",
    "- User defined functions\n",
    "\n",
    "### BigQuery Overview\n",
    "#### BigQuery Benefits:\n",
    "- Interactive analysis of petabyte scale databases\n",
    "- Familiar SQL 2011 query language\n",
    "- Nested and repeat fields, user defined functions in Javascript\n",
    "- Data storage is inexpensive\n",
    "\n",
    "#### BigQuery Sample Architecture\n",
    "##### Project (billing, top-level container)\n",
    "- Limit access to datasets and jobs\n",
    "- Manage billing\n",
    "\n",
    "##### Dataset (organization, access control)\n",
    "- Access Control Lists for Reader/Writer/Owner\n",
    "- Applied to all tables/views in dataset\n",
    "\n",
    "##### Table (data w/ schema)\n",
    "- Columnar storage\n",
    "- Views are in virtual tables defined by SQL query\n",
    "- Tables can be external (Cloud Storage, etc.)\n",
    "- Each column is storage in a separated, encrypted file\n",
    "\n",
    "##### Jobs (query, import, export, copy) \n",
    "- Repeated or long running action\n",
    "- Can be cancelled\n",
    "\n",
    "### Lab: Building a BigQuery Query\n",
    "#### Objectives:\n",
    "- Create and run a query\n",
    "- Modify the query to add clauses, subqueries, built-in functions and joins.\n",
    "\n",
    "#### Task 1: Create and Run a Query\n",
    "- In the Console, on the Products & services menu () click BigQuery. Click on the Compose Query button on top left, and then click on Show Options, and ensure you are using Standard SQL. You are using Standard SQL if the Use Legacy SQL checkbox is unchecked.\n",
    "- Click Hide Options.\n",
    "- In the New Query window, type (or copy-and-paste) the following query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql\n",
    "SELECT\n",
    "  airline,\n",
    "  date,\n",
    "  departure_delay\n",
    "FROM\n",
    "  `bigquery-samples.airline_ontime_data.flights`\n",
    "WHERE\n",
    "  departure_delay > 0\n",
    "  AND departure_airport = 'LGA'\n",
    "LIMIT\n",
    "  100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Aggregate and Boolean Fxns\n",
    "- In the New Query window, type the following query(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of flights departed from LGA\n",
    "SELECT\n",
    "  airline,\n",
    "  COUNT(departure_delay)\n",
    "FROM\n",
    "   `bigquery-samples.airline_ontime_data.flights`\n",
    "WHERE\n",
    "  departure_airport = 'LGA'\n",
    "  AND date = '2008-05-13'\n",
    "GROUP BY\n",
    "  airline\n",
    "ORDER BY airline\n",
    "\n",
    "# total number of late flights from LGA\n",
    "SELECT\n",
    "  airline,\n",
    "  COUNT(departure_delay)\n",
    "FROM\n",
    "   `bigquery-samples.airline_ontime_data.flights`\n",
    "WHERE\n",
    "  departure_delay > 0 AND\n",
    "  departure_airport = 'LGA'\n",
    "  AND date = '2008-05-13'\n",
    "GROUP BY\n",
    "  airline\n",
    "ORDER BY airline\n",
    "\n",
    "# total number of flights AND total delayed flights\n",
    "SELECT\n",
    "  f.airline,\n",
    "  COUNT(f.departure_delay) AS total_flights,\n",
    "  SUM(IF(f.departure_delay > 0, 1, 0)) AS num_delayed\n",
    "FROM\n",
    "   `bigquery-samples.airline_ontime_data.flights` AS f\n",
    "WHERE\n",
    "  f.departure_airport = 'LGA' AND f.date = '2008-05-13'\n",
    "GROUP BY\n",
    "  f.airline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: String Operations, Joins, & Subqueries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT\n",
    "  CONCAT(CAST(year AS STRING), '-', LPAD(CAST(month AS STRING),2,'0'), '-', LPAD(CAST(day AS STRING),2,'0')) AS rainyday\n",
    "FROM\n",
    "  `bigquery-samples.weather_geo.gsod`\n",
    "WHERE\n",
    "  station_number = 725030\n",
    "  AND total_precipitation > 0\n",
    "\n",
    "# join weather data and flight information\n",
    "SELECT\n",
    "  f.airline,\n",
    "  SUM(IF(f.arrival_delay > 0, 1, 0)) AS num_delayed,\n",
    "  COUNT(f.arrival_delay) AS total_flights\n",
    "FROM\n",
    "  `bigquery-samples.airline_ontime_data.flights` AS f\n",
    "JOIN (\n",
    "  SELECT\n",
    "    CONCAT(CAST(year AS STRING), '-', LPAD(CAST(month AS STRING),2,'0'), '-', LPAD(CAST(day AS STRING),2,'0')) AS rainyday\n",
    "  FROM\n",
    "    `bigquery-samples.weather_geo.gsod`\n",
    "  WHERE\n",
    "    station_number = 725030\n",
    "    AND total_precipitation > 0) AS w\n",
    "ON\n",
    "  w.rainyday = f.date\n",
    "WHERE f.arrival_airport = 'LGA'\n",
    "GROUP BY f.airline\n",
    "\n",
    "# fraction of flights delayed per airline\n",
    "SELECT\n",
    "  airline,\n",
    "  num_delayed,\n",
    "  total_flights,\n",
    "  num_delayed / total_flights AS frac_delayed\n",
    "FROM (\n",
    "SELECT\n",
    "  f.airline AS airline,\n",
    "  SUM(IF(f.arrival_delay > 0, 1, 0)) AS num_delayed,\n",
    "  COUNT(f.arrival_delay) AS total_flights\n",
    "FROM\n",
    "  `bigquery-samples.airline_ontime_data.flights` AS f\n",
    "JOIN (\n",
    "  SELECT\n",
    "    CONCAT(CAST(year AS STRING), '-', LPAD(CAST(month AS STRING),2,'0'), '-', LPAD(CAST(day AS STRING),2,'0')) AS rainyday\n",
    "  FROM\n",
    "    `bigquery-samples.weather_geo.gsod`\n",
    "  WHERE\n",
    "    station_number = 725030\n",
    "    AND total_precipitation > 0) AS w\n",
    "ON\n",
    "  w.rainyday = f.date\n",
    "WHERE f.arrival_airport = 'LGA'\n",
    "GROUP BY f.airline\n",
    "  )\n",
    "ORDER BY\n",
    "  frac_delayed ASC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Lab\n",
    "### Lab: Loading & Exporting Data\n",
    "#### Objectives:\n",
    "- Load a CSV file into a BigQuery table using the web UI\n",
    "- Load a JSON file into a BigQuery table using the CLI\n",
    "- Export a table using the web UI\n",
    "\n",
    "#### Task 1: Upload the data using the web UI\n",
    "\n",
    "- Return to the browser tab containing Console.\n",
    "- In the Console, on the Products & services menu () click BigQuery.\n",
    "- In the left column, beneath the text box, find your project name. To the right of the project name, click the blue arrow. - Choose Create new dataset.\n",
    "- In the â€˜Create Dataset' dialog, for Dataset ID, type cpb101_flight_data and click OK.\n",
    "- Download the following file to your local machine. This file contains the data that will populate the first table.\n",
    "\n",
    "[Download airports.csv](https://storage.googleapis.com/cloud-training/CPB200/BQ/lab4/airports.csv)\n",
    "\n",
    "- Create a new table in the cpb101_flight_data dataset to store the data from the CSV file. Mouse over the line with the name of the dataset. This will reveal the create table icon, which looks like a plus sign. Click the create table icon (the plus sign) to the right of the cpb101_flight_data dataset.\n",
    "- On the Create Table page, in the Source Data section:\n",
    "    - For Location, leave File upload selected.\n",
    "    - To the right of File upload, click Choose file, then browse to and select airports.csv.\n",
    "    - Verify File format is set to CSV.\n",
    "In the Destination Table section:\n",
    "    - For Table name, leave cpb101_flight_data selected.\n",
    "    - For Destination table name, type AIRPORTS.\n",
    "    - For Table type, Native table should be selected and unchangeable.\n",
    "- In the Schema section:\n",
    "- Add fields one at a time. The airports.csv has the following fields: IATA, AIRPORT, CITY, STATE, COUNTRY which are of type STRING and LATITUDE, LONGITUDE which are of type FLOAT. Make all these fields REQUIRED.\n",
    "- In the Options section:\n",
    "    - For Field delimiter, verify Comma is selected.\n",
    "    - Since airports.csv contains a single header row, for Header rows to skip, type 1.\n",
    "    - Accept the remaining default values and click Create Table. BigQuery creates a load job to create the table and upload data into the table (this may take a few seconds). You can track job progress by clicking Job History\n",
    "- Once the load job is complete, click cpb101_flight_data > AIRPORTS.\n",
    "- On the Table Details page, click Details to view the table properties and then click Preview to view the table data.\n",
    "\n",
    "#### Task 2. Upload the data using the CLI\n",
    "- Return to the browser tab containing Cloud Shell.\n",
    "- In Cloud Shell, enter the following command to download the schema file for the table to your working directory. (The file is schema_flight_performance.json)\n",
    "``` \n",
    "curl https://storage.googleapis.com/cloud-training/CPB200/BQ/lab4/schema_flight_performance.json -o schema_flight_performance.json\n",
    "```\n",
    "\n",
    "- Next, you will create a table in the dataset using the schema file you downloaded to Cloud Shell and data from JSON files that are in Cloud Storage. The JSON files have URIs like the following:\n",
    "```\n",
    "gs://cloud-training/CPB200/BQ/lab4/domestic_2014_flights_*.json\n",
    "```\n",
    "\n",
    "##### Note that your Project ID is stored as a variable in Cloud Shell ($DEVSHELL_PROJECT_ID) so there's no need for you to remember it. If you require it, you can view your Project ID in the command line to the right of your username (after the @ symbol).\n",
    "\n",
    "- In Cloud Shell, create a table named flights_2014 in the cpb101_flight_data dataset with this command:\n",
    "\n",
    "``` \n",
    "bq load --source_format=NEWLINE_DELIMITED_JSON $DEVSHELL_PROJECT_ID:cpb101_flight_data.flights_2014 gs://cloud-training/CPB200/BQ/lab4/domestic_2014_flights_*.json ./schema_flight_performance.json \n",
    "\n",
    "```\n",
    "- There are multiple JSON files in the Cloud Storage bucket. They are named according to the convention: domestic_2014_flights_*.json. The wildcard (*) character in the command is used to include all of the .json files in the bucket.\n",
    "- Once the table is created, type the following command to verify table flights_2014 exists in dataset cpb101_flight_data:\n",
    "\n",
    "```\n",
    "bq ls $DEVSHELL_PROJECT_ID:cpb101_flight_data\n",
    "```\n",
    "\n",
    "#### Task 3: Export table\n",
    "- In the Console, on the Products & services menu () click Home\n",
    "- Select and copy the Project ID. For simplicity you will use the Qwiklabs Project ID, which is already globally unique, as the bucket name.\n",
    "- In the Console, on the Products & services menu () click Storage > Browser.\n",
    "- Click Create Bucket.\n",
    "- Specify the following, and leave the remaining settings as their defaults:\n",
    "- Click Create.\n",
    "- Record the name of your bucket. You will need it in subsequent tasks.\n",
    "- In Cloud Shell enter the following to create an environment variable named \"BUCKET\" and verify that it exists with the echo command.\n",
    "\n",
    "```\n",
    "BUCKET= |your unique bucket name (Project ID)|\n",
    "\n",
    "echo $BUCKET\n",
    "```\n",
    "\n",
    "- You can use BUCKET in Cloud Shell commands. And if you need to enter the bucket name <your-bucket> in a text field in Console, you can quickly retrieve the name with \"echo $BUCKET\".\n",
    "- Return to the BigQuery web UI. If it is not already open, open Console. On the Products & services menu () click BigQuery.\n",
    "- Select the AIRPORTS table that you created recently, and using the \"down\" button to its right, select the option for Export Table.\n",
    "- In the dialog, specify gs://<YOUR-BUCKET>/bq/airports.csv and click OK.\n",
    "- Use the CLI to export the table:\n",
    "\n",
    "```\n",
    "bq extract cpb101_flight_data.AIRPORTS gs://$BUCKET/bq/airports2.csv\n",
    "```\n",
    "\n",
    "- In the Console, on the Products & services menu () click Storage > Browser. Browse to your bucket and ensure that both .csv files have been created.\n",
    "\n",
    "### End Lab\n",
    "\n",
    "### Lab: Advanced SQL Queries\n",
    "#### Objectives: \n",
    "- Use Nested fields, Regular expressions, With statement, and Group and Having\n",
    "- Extract programming information about code commits\n",
    "\n",
    "#### Task 1: Get information about code commits\n",
    "- In the Console, on the Products & services menu () click BigQuery.\n",
    "- Compose a new query, making sure that the \"Legacy SQL\" option is not checked (you are using Standard SQL).\n",
    "```\n",
    "SELECT\n",
    "  author.email,\n",
    "  diff.new_path AS path,\n",
    "  author.date\n",
    "FROM\n",
    "  `bigquery-public-data.github_repos.commits`,\n",
    "  UNNEST(difference) diff\n",
    "WHERE\n",
    "  EXTRACT(YEAR\n",
    "  FROM\n",
    "    author.date)=2016\n",
    "LIMIT 10\n",
    "```\n",
    "- Play a little with the query above to understand what it is doing. For example, instead of author.email, try just author. What type of field is author?\n",
    "- Change diff.new_path to difference.new_path. Why does it not work? Replace difference.new_path by difference[OFFSET(0)].new_path. Does this work? Why? What is the UNNEST doing?\n",
    "\n",
    "#### Task 2: Extract programming language\n",
    "- Run the following query. Remember to uncheck Legacy SQL.\n",
    "\n",
    "```\n",
    "SELECT\n",
    "  author.email,\n",
    "  LOWER(REGEXP_EXTRACT(diff.new_path, r'\\.([^\\./\\(~_ \\- #]*)$')) lang,\n",
    "  diff.new_path AS path,\n",
    "  author.date\n",
    "FROM\n",
    "  `bigquery-public-data.github_repos.commits`,\n",
    "  UNNEST(difference) diff\n",
    "WHERE\n",
    "  EXTRACT(YEAR\n",
    "  FROM\n",
    "    author.date)=2016\n",
    "LIMIT\n",
    "  10\n",
    "```\n",
    "- Modify the query above to only use lang if the language consists purely of letters and has a length that is fewer than 8 characters.\n",
    "- Modify the query above to group by language and list in descending order of the number of commits.\n",
    "- Here is one solution:\n",
    "```\n",
    "WITH\n",
    "  commits AS (\n",
    "  SELECT\n",
    "    author.email,\n",
    "    LOWER(REGEXP_EXTRACT(diff.new_path, r'\\.([^\\./\\(~_ \\- #]*)$')) lang,\n",
    "    diff.new_path AS path,\n",
    "    author.date\n",
    "  FROM\n",
    "    `bigquery-public-data.github_repos.commits`,\n",
    "    UNNEST(difference) diff\n",
    "  WHERE\n",
    "    EXTRACT(YEAR\n",
    "    FROM\n",
    "      author.date)=2016 )\n",
    "SELECT\n",
    "  lang,\n",
    "  COUNT(path) AS numcommits\n",
    "FROM\n",
    "  commits\n",
    "WHERE\n",
    "  LENGTH(lang)<8\n",
    "  AND lang IS NOT NULL\n",
    "  AND REGEXP_CONTAINS(lang, '[a-zA-Z]')\n",
    "GROUP BY\n",
    "  lang\n",
    "HAVING\n",
    "  numcommits > 100\n",
    "ORDER BY\n",
    "  numcommits DESC\n",
    "```\n",
    "\n",
    "#### Task 3: Weekend or weekday?\n",
    "- Modify the previous query to extract the day of the week from author.date. Days 2 to 6 are weekdays.\n",
    "```\n",
    "WITH\n",
    "  commits AS (\n",
    "  SELECT\n",
    "    author.email,\n",
    "    EXTRACT(DAYOFWEEK\n",
    "    FROM\n",
    "      author.date) BETWEEN 2\n",
    "    AND 6 is_weekday,\n",
    "    LOWER(REGEXP_EXTRACT(diff.new_path, r'\\.([^\\./\\(~_ \\- #]*)$')) lang,\n",
    "    diff.new_path AS path,\n",
    "    author.date\n",
    "  FROM\n",
    "    `bigquery-public-data.github_repos.commits`,\n",
    "    UNNEST(difference) diff\n",
    "  WHERE\n",
    "    EXTRACT(YEAR\n",
    "    FROM\n",
    "      author.date)=2016)\n",
    "SELECT\n",
    "  lang,\n",
    "  is_weekday,\n",
    "  COUNT(path) AS numcommits\n",
    "FROM\n",
    "  commits\n",
    "WHERE\n",
    "  lang IS NOT NULL\n",
    "GROUP BY\n",
    "  lang,\n",
    "  is_weekday\n",
    "HAVING\n",
    "  numcommits > 100\n",
    "ORDER BY\n",
    "  numcommits DESC\n",
    "```\n",
    "#### Note: Ignoring file extensions that do not correspond to programming languages, it appears that the most popular weekend programming languages are JavaScript, PHP and C (*.h is the C header file), Java, and Python.\n",
    "\n",
    "### End Lab\n",
    "\n",
    "### Module 1 Review\n",
    "\n",
    "1.) I want to query a table, then query within the results of that query. Which of these is the BEST way to do this?\n",
    "- Use a subquery of the form SELECT ... FROM(SELECT) ...\n",
    "\n",
    "2.) Which of the following statements are true?\n",
    "- Dataflow transforms both batch and streaming pipelines\n",
    "- Dataflow executes Apache Beam pipelines\n",
    "\n",
    "Incorrect: \n",
    "- Side-inputs in Dataflow are a way to export data from one pipeline to share with another pipeline\n",
    "- Map operations in MapReduce and be performed by Combine transforms in Dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 2: Autoscaling Data Processing Pipelines with Dataflow\n",
    "#### Topics:\n",
    "- Pipeline concepts\n",
    "- MapReduce\n",
    "- Side inputs\n",
    "- Streaming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
