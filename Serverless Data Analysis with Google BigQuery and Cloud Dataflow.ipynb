{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCP Professional Data Engineer\n",
    "### Serverless Data Analysis with Google BigQuery and Cloud Dataflow\n",
    "#### Modules:\n",
    "- Serverless Data Analysis with BigQuery\n",
    "- Autoscaling Data Processing Pipelines with Dataflow\n",
    "\n",
    "#### Learning Objectives:\n",
    "\n",
    "- Build up a complex BigQuery using clauses, inner selects, built-in functions and joins\n",
    "- Load and export data to/from BigQuery\n",
    "- Identify need for nested, repeated fields and user-defined functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 1: Serverless Data Analysis with Big Query\n",
    "#### Topics:\n",
    "- Queries\n",
    "- Functions\n",
    "- Load & export data\n",
    "- Nested, repeated fields\n",
    "- Window functions\n",
    "- User defined functions\n",
    "\n",
    "### BigQuery Overview\n",
    "#### BigQuery Benefits:\n",
    "- Interactive analysis of petabyte scale databases\n",
    "- Familiar SQL 2011 query language\n",
    "- Nested and repeat fields, user defined functions in Javascript\n",
    "- Data storage is inexpensive\n",
    "\n",
    "#### BigQuery Sample Architecture\n",
    "##### Project (billing, top-level container)\n",
    "- Limit access to datasets and jobs\n",
    "- Manage billing\n",
    "\n",
    "##### Dataset (organization, access control)\n",
    "- Access Control Lists for Reader/Writer/Owner\n",
    "- Applied to all tables/views in dataset\n",
    "\n",
    "##### Table (data w/ schema)\n",
    "- Columnar storage\n",
    "- Views are in virtual tables defined by SQL query\n",
    "- Tables can be external (Cloud Storage, etc.)\n",
    "- Each column is storage in a separated, encrypted file\n",
    "\n",
    "##### Jobs (query, import, export, copy) \n",
    "- Repeated or long running action\n",
    "- Can be cancelled\n",
    "\n",
    "### Lab: Building a BigQuery Query\n",
    "#### Objectives:\n",
    "- Create and run a query\n",
    "- Modify the query to add clauses, subqueries, built-in functions and joins.\n",
    "\n",
    "#### Task 1: Create and Run a Query\n",
    "- In the Console, on the Products & services menu () click BigQuery. Click on the Compose Query button on top left, and then click on Show Options, and ensure you are using Standard SQL. You are using Standard SQL if the Use Legacy SQL checkbox is unchecked.\n",
    "- Click Hide Options.\n",
    "- In the New Query window, type (or copy-and-paste) the following query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql\n",
    "SELECT\n",
    "  airline,\n",
    "  date,\n",
    "  departure_delay\n",
    "FROM\n",
    "  `bigquery-samples.airline_ontime_data.flights`\n",
    "WHERE\n",
    "  departure_delay > 0\n",
    "  AND departure_airport = 'LGA'\n",
    "LIMIT\n",
    "  100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Aggregate and Boolean Fxns\n",
    "- In the New Query window, type the following query(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of flights departed from LGA\n",
    "SELECT\n",
    "  airline,\n",
    "  COUNT(departure_delay)\n",
    "FROM\n",
    "   `bigquery-samples.airline_ontime_data.flights`\n",
    "WHERE\n",
    "  departure_airport = 'LGA'\n",
    "  AND date = '2008-05-13'\n",
    "GROUP BY\n",
    "  airline\n",
    "ORDER BY airline\n",
    "\n",
    "# total number of late flights from LGA\n",
    "SELECT\n",
    "  airline,\n",
    "  COUNT(departure_delay)\n",
    "FROM\n",
    "   `bigquery-samples.airline_ontime_data.flights`\n",
    "WHERE\n",
    "  departure_delay > 0 AND\n",
    "  departure_airport = 'LGA'\n",
    "  AND date = '2008-05-13'\n",
    "GROUP BY\n",
    "  airline\n",
    "ORDER BY airline\n",
    "\n",
    "# total number of flights AND total delayed flights\n",
    "SELECT\n",
    "  f.airline,\n",
    "  COUNT(f.departure_delay) AS total_flights,\n",
    "  SUM(IF(f.departure_delay > 0, 1, 0)) AS num_delayed\n",
    "FROM\n",
    "   `bigquery-samples.airline_ontime_data.flights` AS f\n",
    "WHERE\n",
    "  f.departure_airport = 'LGA' AND f.date = '2008-05-13'\n",
    "GROUP BY\n",
    "  f.airline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: String Operations, Joins, & Subqueries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT\n",
    "  CONCAT(CAST(year AS STRING), '-', LPAD(CAST(month AS STRING),2,'0'), '-', LPAD(CAST(day AS STRING),2,'0')) AS rainyday\n",
    "FROM\n",
    "  `bigquery-samples.weather_geo.gsod`\n",
    "WHERE\n",
    "  station_number = 725030\n",
    "  AND total_precipitation > 0\n",
    "\n",
    "# join weather data and flight information\n",
    "SELECT\n",
    "  f.airline,\n",
    "  SUM(IF(f.arrival_delay > 0, 1, 0)) AS num_delayed,\n",
    "  COUNT(f.arrival_delay) AS total_flights\n",
    "FROM\n",
    "  `bigquery-samples.airline_ontime_data.flights` AS f\n",
    "JOIN (\n",
    "  SELECT\n",
    "    CONCAT(CAST(year AS STRING), '-', LPAD(CAST(month AS STRING),2,'0'), '-', LPAD(CAST(day AS STRING),2,'0')) AS rainyday\n",
    "  FROM\n",
    "    `bigquery-samples.weather_geo.gsod`\n",
    "  WHERE\n",
    "    station_number = 725030\n",
    "    AND total_precipitation > 0) AS w\n",
    "ON\n",
    "  w.rainyday = f.date\n",
    "WHERE f.arrival_airport = 'LGA'\n",
    "GROUP BY f.airline\n",
    "\n",
    "# fraction of flights delayed per airline\n",
    "SELECT\n",
    "  airline,\n",
    "  num_delayed,\n",
    "  total_flights,\n",
    "  num_delayed / total_flights AS frac_delayed\n",
    "FROM (\n",
    "SELECT\n",
    "  f.airline AS airline,\n",
    "  SUM(IF(f.arrival_delay > 0, 1, 0)) AS num_delayed,\n",
    "  COUNT(f.arrival_delay) AS total_flights\n",
    "FROM\n",
    "  `bigquery-samples.airline_ontime_data.flights` AS f\n",
    "JOIN (\n",
    "  SELECT\n",
    "    CONCAT(CAST(year AS STRING), '-', LPAD(CAST(month AS STRING),2,'0'), '-', LPAD(CAST(day AS STRING),2,'0')) AS rainyday\n",
    "  FROM\n",
    "    `bigquery-samples.weather_geo.gsod`\n",
    "  WHERE\n",
    "    station_number = 725030\n",
    "    AND total_precipitation > 0) AS w\n",
    "ON\n",
    "  w.rainyday = f.date\n",
    "WHERE f.arrival_airport = 'LGA'\n",
    "GROUP BY f.airline\n",
    "  )\n",
    "ORDER BY\n",
    "  frac_delayed ASC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Lab\n",
    "### Lab: Loading & Exporting Data\n",
    "#### Objectives:\n",
    "- Load a CSV file into a BigQuery table using the web UI\n",
    "- Load a JSON file into a BigQuery table using the CLI\n",
    "- Export a table using the web UI\n",
    "\n",
    "#### Task 1: Upload the data using the web UI\n",
    "\n",
    "- Return to the browser tab containing Console.\n",
    "- In the Console, on the Products & services menu () click BigQuery.\n",
    "- In the left column, beneath the text box, find your project name. To the right of the project name, click the blue arrow. - Choose Create new dataset.\n",
    "- In the â€˜Create Dataset' dialog, for Dataset ID, type cpb101_flight_data and click OK.\n",
    "- Download the following file to your local machine. This file contains the data that will populate the first table.\n",
    "\n",
    "[Download airports.csv](https://storage.googleapis.com/cloud-training/CPB200/BQ/lab4/airports.csv)\n",
    "\n",
    "- Create a new table in the cpb101_flight_data dataset to store the data from the CSV file. Mouse over the line with the name of the dataset. This will reveal the create table icon, which looks like a plus sign. Click the create table icon (the plus sign) to the right of the cpb101_flight_data dataset.\n",
    "- On the Create Table page, in the Source Data section:\n",
    "    - For Location, leave File upload selected.\n",
    "    - To the right of File upload, click Choose file, then browse to and select airports.csv.\n",
    "    - Verify File format is set to CSV.\n",
    "In the Destination Table section:\n",
    "    - For Table name, leave cpb101_flight_data selected.\n",
    "    - For Destination table name, type AIRPORTS.\n",
    "    - For Table type, Native table should be selected and unchangeable.\n",
    "- In the Schema section:\n",
    "- Add fields one at a time. The airports.csv has the following fields: IATA, AIRPORT, CITY, STATE, COUNTRY which are of type STRING and LATITUDE, LONGITUDE which are of type FLOAT. Make all these fields REQUIRED.\n",
    "- In the Options section:\n",
    "    - For Field delimiter, verify Comma is selected.\n",
    "    - Since airports.csv contains a single header row, for Header rows to skip, type 1.\n",
    "    - Accept the remaining default values and click Create Table. BigQuery creates a load job to create the table and upload data into the table (this may take a few seconds). You can track job progress by clicking Job History\n",
    "- Once the load job is complete, click cpb101_flight_data > AIRPORTS.\n",
    "- On the Table Details page, click Details to view the table properties and then click Preview to view the table data.\n",
    "\n",
    "#### Task 2. Upload the data using the CLI\n",
    "- Return to the browser tab containing Cloud Shell.\n",
    "- In Cloud Shell, enter the following command to download the schema file for the table to your working directory. (The file is schema_flight_performance.json)\n",
    "``` \n",
    "curl https://storage.googleapis.com/cloud-training/CPB200/BQ/lab4/schema_flight_performance.json -o schema_flight_performance.json\n",
    "```\n",
    "\n",
    "- Next, you will create a table in the dataset using the schema file you downloaded to Cloud Shell and data from JSON files that are in Cloud Storage. The JSON files have URIs like the following:\n",
    "```\n",
    "gs://cloud-training/CPB200/BQ/lab4/domestic_2014_flights_*.json\n",
    "```\n",
    "\n",
    "##### Note that your Project ID is stored as a variable in Cloud Shell ($DEVSHELL_PROJECT_ID) so there's no need for you to remember it. If you require it, you can view your Project ID in the command line to the right of your username (after the @ symbol).\n",
    "\n",
    "- In Cloud Shell, create a table named flights_2014 in the cpb101_flight_data dataset with this command:\n",
    "\n",
    "``` \n",
    "bq load --source_format=NEWLINE_DELIMITED_JSON $DEVSHELL_PROJECT_ID:cpb101_flight_data.flights_2014 gs://cloud-training/CPB200/BQ/lab4/domestic_2014_flights_*.json ./schema_flight_performance.json \n",
    "\n",
    "```\n",
    "- There are multiple JSON files in the Cloud Storage bucket. They are named according to the convention: domestic_2014_flights_*.json. The wildcard (*) character in the command is used to include all of the .json files in the bucket.\n",
    "- Once the table is created, type the following command to verify table flights_2014 exists in dataset cpb101_flight_data:\n",
    "\n",
    "```\n",
    "bq ls $DEVSHELL_PROJECT_ID:cpb101_flight_data\n",
    "```\n",
    "\n",
    "#### Task 3: Export table\n",
    "- In the Console, on the Products & services menu () click Home\n",
    "- Select and copy the Project ID. For simplicity you will use the Qwiklabs Project ID, which is already globally unique, as the bucket name.\n",
    "- In the Console, on the Products & services menu () click Storage > Browser.\n",
    "- Click Create Bucket.\n",
    "- Specify the following, and leave the remaining settings as their defaults:\n",
    "- Click Create.\n",
    "- Record the name of your bucket. You will need it in subsequent tasks.\n",
    "- In Cloud Shell enter the following to create an environment variable named \"BUCKET\" and verify that it exists with the echo command.\n",
    "\n",
    "```\n",
    "BUCKET= |your unique bucket name (Project ID)|\n",
    "\n",
    "echo $BUCKET\n",
    "```\n",
    "\n",
    "- You can use BUCKET in Cloud Shell commands. And if you need to enter the bucket name <your-bucket> in a text field in Console, you can quickly retrieve the name with \"echo $BUCKET\".\n",
    "- Return to the BigQuery web UI. If it is not already open, open Console. On the Products & services menu () click BigQuery.\n",
    "- Select the AIRPORTS table that you created recently, and using the \"down\" button to its right, select the option for Export Table.\n",
    "- In the dialog, specify gs://<YOUR-BUCKET>/bq/airports.csv and click OK.\n",
    "- Use the CLI to export the table:\n",
    "\n",
    "```\n",
    "bq extract cpb101_flight_data.AIRPORTS gs://$BUCKET/bq/airports2.csv\n",
    "```\n",
    "\n",
    "- In the Console, on the Products & services menu () click Storage > Browser. Browse to your bucket and ensure that both .csv files have been created.\n",
    "\n",
    "### End Lab\n",
    "\n",
    "### Lab: Advanced SQL Queries\n",
    "#### Objectives: \n",
    "- Use Nested fields, Regular expressions, With statement, and Group and Having\n",
    "- Extract programming information about code commits\n",
    "\n",
    "#### Task 1: Get information about code commits\n",
    "- In the Console, on the Products & services menu () click BigQuery.\n",
    "- Compose a new query, making sure that the \"Legacy SQL\" option is not checked (you are using Standard SQL).\n",
    "```\n",
    "SELECT\n",
    "  author.email,\n",
    "  diff.new_path AS path,\n",
    "  author.date\n",
    "FROM\n",
    "  `bigquery-public-data.github_repos.commits`,\n",
    "  UNNEST(difference) diff\n",
    "WHERE\n",
    "  EXTRACT(YEAR\n",
    "  FROM\n",
    "    author.date)=2016\n",
    "LIMIT 10\n",
    "```\n",
    "- Play a little with the query above to understand what it is doing. For example, instead of author.email, try just author. What type of field is author?\n",
    "- Change diff.new_path to difference.new_path. Why does it not work? Replace difference.new_path by difference[OFFSET(0)].new_path. Does this work? Why? What is the UNNEST doing?\n",
    "\n",
    "#### Task 2: Extract programming language\n",
    "- Run the following query. Remember to uncheck Legacy SQL.\n",
    "\n",
    "```\n",
    "SELECT\n",
    "  author.email,\n",
    "  LOWER(REGEXP_EXTRACT(diff.new_path, r'\\.([^\\./\\(~_ \\- #]*)$')) lang,\n",
    "  diff.new_path AS path,\n",
    "  author.date\n",
    "FROM\n",
    "  `bigquery-public-data.github_repos.commits`,\n",
    "  UNNEST(difference) diff\n",
    "WHERE\n",
    "  EXTRACT(YEAR\n",
    "  FROM\n",
    "    author.date)=2016\n",
    "LIMIT\n",
    "  10\n",
    "```\n",
    "- Modify the query above to only use lang if the language consists purely of letters and has a length that is fewer than 8 characters.\n",
    "- Modify the query above to group by language and list in descending order of the number of commits.\n",
    "- Here is one solution:\n",
    "```\n",
    "WITH\n",
    "  commits AS (\n",
    "  SELECT\n",
    "    author.email,\n",
    "    LOWER(REGEXP_EXTRACT(diff.new_path, r'\\.([^\\./\\(~_ \\- #]*)$')) lang,\n",
    "    diff.new_path AS path,\n",
    "    author.date\n",
    "  FROM\n",
    "    `bigquery-public-data.github_repos.commits`,\n",
    "    UNNEST(difference) diff\n",
    "  WHERE\n",
    "    EXTRACT(YEAR\n",
    "    FROM\n",
    "      author.date)=2016 )\n",
    "SELECT\n",
    "  lang,\n",
    "  COUNT(path) AS numcommits\n",
    "FROM\n",
    "  commits\n",
    "WHERE\n",
    "  LENGTH(lang)<8\n",
    "  AND lang IS NOT NULL\n",
    "  AND REGEXP_CONTAINS(lang, '[a-zA-Z]')\n",
    "GROUP BY\n",
    "  lang\n",
    "HAVING\n",
    "  numcommits > 100\n",
    "ORDER BY\n",
    "  numcommits DESC\n",
    "```\n",
    "\n",
    "#### Task 3: Weekend or weekday?\n",
    "- Modify the previous query to extract the day of the week from author.date. Days 2 to 6 are weekdays.\n",
    "```\n",
    "WITH\n",
    "  commits AS (\n",
    "  SELECT\n",
    "    author.email,\n",
    "    EXTRACT(DAYOFWEEK\n",
    "    FROM\n",
    "      author.date) BETWEEN 2\n",
    "    AND 6 is_weekday,\n",
    "    LOWER(REGEXP_EXTRACT(diff.new_path, r'\\.([^\\./\\(~_ \\- #]*)$')) lang,\n",
    "    diff.new_path AS path,\n",
    "    author.date\n",
    "  FROM\n",
    "    `bigquery-public-data.github_repos.commits`,\n",
    "    UNNEST(difference) diff\n",
    "  WHERE\n",
    "    EXTRACT(YEAR\n",
    "    FROM\n",
    "      author.date)=2016)\n",
    "SELECT\n",
    "  lang,\n",
    "  is_weekday,\n",
    "  COUNT(path) AS numcommits\n",
    "FROM\n",
    "  commits\n",
    "WHERE\n",
    "  lang IS NOT NULL\n",
    "GROUP BY\n",
    "  lang,\n",
    "  is_weekday\n",
    "HAVING\n",
    "  numcommits > 100\n",
    "ORDER BY\n",
    "  numcommits DESC\n",
    "```\n",
    "#### Note: Ignoring file extensions that do not correspond to programming languages, it appears that the most popular weekend programming languages are JavaScript, PHP and C (*.h is the C header file), Java, and Python.\n",
    "\n",
    "### End Lab\n",
    "\n",
    "### Module 1 Review\n",
    "\n",
    "1.) I want to query a table, then query within the results of that query. Which of these is the BEST way to do this?\n",
    "- Use a subquery of the form SELECT ... FROM(SELECT) ...\n",
    "\n",
    "2.) Which of the following statements are true?\n",
    "- Dataflow transforms both batch and streaming pipelines\n",
    "- Dataflow executes Apache Beam pipelines\n",
    "\n",
    "Incorrect: \n",
    "- Side-inputs in Dataflow are a way to export data from one pipeline to share with another pipeline\n",
    "- Map operations in MapReduce and be performed by Combine transforms in Dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 2: Autoscaling Data Processing Pipelines with Dataflow\n",
    "#### Topics:\n",
    "- Pipeline concepts\n",
    "- MapReduce\n",
    "- Side inputs\n",
    "- Streaming\n",
    "\n",
    "### Lab: A Simple Dataflow Pipeline (Python)\n",
    "\n",
    "#### Objectives:\n",
    "-Setup a Python Dataflow project using Apache Beam\n",
    "-Write a simple pipeline in Python\n",
    "-Execute the query on the local machine\n",
    "-Execute the query on the cloud\n",
    "\n",
    "#### Task 1: Preparation\n",
    "- Verify that the repository exists, and if not, clone it. Return to the browser tab containing the Cloud Shell code editor. Click on File > Refresh in the left navigator panel. You should see the training-data-analyst directory.\n",
    "- If the directory does not exist clone the repository from the Cloud Shell command line:\n",
    "```\n",
    "cd ~\n",
    "git clone https://github.com/GoogleCloudPlatform/training-data-analyst\n",
    "```\n",
    "- In the Console, on the Products & services menu () click Home\n",
    "-Select and copy the Project ID. For simplicity you will use the Qwiklabs Project ID, which is already globally unique, as the bucket name.\n",
    "- In the Console, on the Products & services menu () click Storage > Browser.\n",
    "- Click Create Bucket.\n",
    "- Record the name of your bucket. You will need it in subsequent tasks.\n",
    "- In Cloud Shell enter the following to create an environment variable named \"BUCKET\" and verify that it exists with the echo command.\n",
    "```\n",
    "BUCKET=\"<your unique bucket name (Project ID)>\"\n",
    "echo (dollarsign)BUCKET\n",
    "```\n",
    "\n",
    "- Return to the browser tab for Console. In the top search bar, enter Google Dataflow API. This will take you to the page, Products & Services > APIs & Services > Dashboard > Google Dataflow API. It will either show a status information or it will give you the option to Enable the API.\n",
    "- If necessary, Enable the API.\n",
    "\n",
    "#### Task 2: Open Dataflow Project\n",
    "- The goal of this lab is to become familiar with the structure of a Dataflow project and learn how to execute a Dataflow pipeline. You will need to update some files to install Apache Beam. Apache Beam is an open source platform for executing data processing workflows.\n",
    "- Return to the browser tab containing Cloud Shell. In Cloud Shell navigate to the directory for this lab:\n",
    "```\n",
    "cd ~/training-data-analyst/courses/data_analysis/lab2/python\n",
    "# install dependencies\n",
    "sudo ./install_packages.sh\n",
    "# check version\n",
    "pip -V\n",
    "```\n",
    "#### Task 3: Pipeline Filtering\n",
    "In the Cloud Shell code editor navigate to the directory /training-data-analyst/courses/data_analysis/lab2/python and view the file grep.py Do not make any changes to the code.\n",
    "```\n",
    "cd ~/training-data-analyst/courses/data_analysis/lab2/python\n",
    "nano grep.py\n",
    "```\n",
    "\n",
    "[This file can be found here.](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/data_analysis/lab2/python/grep.py)\n",
    "\n",
    "- Can you answer these questions about the file grep.py?\n",
    "\n",
    "    - What files are being read?\n",
    "    - What is the search term?\n",
    "    - Where does the output go?\n",
    "    - There are three transforms in the pipeline:\n",
    "\n",
    "    - What does the transform do?\n",
    "    - What does the second transform do?\n",
    "    - Where does its input come from?\n",
    "    - What does it do with this input?\n",
    "    - What does it write to its output?\n",
    "    - Where does the output go to?\n",
    "    - What does the third transform do?\n",
    "    \n",
    "#### Task 4: Execute the Pipeline Locally\n",
    "\n",
    "- In the Cloud Shell command line, locally execute grep.py\n",
    "```\n",
    "cd ~/training-data-analyst/courses/data_analysis/lab2/python\n",
    "python grep.py\n",
    "```\n",
    "- The output file will be output.txt. If the output is large enough, it will be sharded into separate parts with names like: output-00000-of-00001. If necessary, you can locate the correct file by examining the file's time.\n",
    "```\n",
    "# examine file's time\n",
    "ls -al /tmp\n",
    "# examine output file\n",
    "cat /tmp/output-*\n",
    "```\n",
    "\n",
    "#### Task 5: Execute the pipeline on the cloud\n",
    "\n",
    "- Copy some Java files to the cloud\n",
    "```\n",
    "gsutil cp ../javahelp/src/main/java/com/google/cloud/training/dataanalyst/javahelp/*.java gs://(dollarsign)BUCKET/javahelp\n",
    "```\n",
    "- Edit the Dataflow pipeline in grepc.py. In the Cloud Shell code editor navigate to the directory /training-data-analyst/courses/data_analysis/lab2/python in and edit the file grepc.py\n",
    "- Replace PROJECT and BUCKET with your Project ID and Bucket name. Here are easy ways to retrieve the values:\n",
    "\n",
    "[This pipeline can be found here: grepc.py](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/data_analysis/lab2/python/grepc.py)\n",
    "\n",
    "echo (dollarsign)DEVSHELL_PROJECT_ID\n",
    "echo (dollarsign)BUCKET\n",
    "\n",
    "- Submit dataflow job to the cloud\n",
    "```\n",
    "python grepc.py\n",
    "```\n",
    "\n",
    "- Return to the browser tab for Console. On the Products & services menu () click Dataflow and click on your job to monitor progress.\n",
    "- Wait for the job status to turn to Succeeded. At this point, your Cloud Shell will display a command-line prompt.\n",
    "- Examine the output in the Cloud Storage bucket. On the Products & services menu () click Storage > Browser and click on your bucket. Click the javahelp directory. This job will generate the file output.txt. If the file is large enough it will be sharded into multiple parts with names like: output-0000x-of-000y. You can identify the most recent file by name or by the Last modified field. Click on the file to view it.\n",
    "- Alternatively, you could download the file in Cloud Shell and view it:\n",
    "\n",
    "```\n",
    "gsutil cp gs://$BUCKET/javahelp/output.txt .\n",
    "cat output.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'apache_beam'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-997a2e2cdd53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \"\"\"\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mapache_beam\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbeam\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'apache_beam'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "Copyright Google Inc. 2016\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "import apache_beam as beam\n",
    "import re\n",
    "import sys\n",
    "\n",
    "def my_grep(line, term):\n",
    "   if re.match( r'^' + re.escape(term), line):\n",
    "      yield line\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   p = beam.Pipeline(argv=sys.argv)\n",
    "   input = '../javahelp/src/main/java/com/google/cloud/training/dataanalyst/javahelp/*.java'\n",
    "   output_prefix = '/tmp/output'\n",
    "   searchTerm = 'import'\n",
    "\n",
    "   # find all lines that contain the searchTerm\n",
    "   (p\n",
    "      | 'GetJava' >> beam.io.ReadFromText(input)\n",
    "      | 'Grep' >> beam.FlatMap(lambda line: my_grep(line, searchTerm) )\n",
    "      | 'write' >> beam.io.WriteToText(output_prefix)\n",
    "   )\n",
    "\n",
    "   p.run().wait_until_finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "Copyright Google Inc. 2016\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "import apache_beam as beam\n",
    "import re\n",
    "\n",
    "def my_grep(line, term):\n",
    "   if re.match( r'^' + re.escape(term), line):\n",
    "      yield line\n",
    "\n",
    "PROJECT='cloud-training-demos'\n",
    "BUCKET='cloud-training-demos'\n",
    "\n",
    "def run():\n",
    "   argv = [\n",
    "      '--project={0}'.format(PROJECT),\n",
    "      '--job_name=examplejob2',\n",
    "      '--save_main_session',\n",
    "      '--staging_location=gs://{0}/staging/'.format(BUCKET),\n",
    "      '--temp_location=gs://{0}/staging/'.format(BUCKET),\n",
    "      '--runner=DataflowRunner'\n",
    "   ]\n",
    "\n",
    "   p = beam.Pipeline(argv=argv)\n",
    "   input = 'gs://{0}/javahelp/*.java'.format(BUCKET)\n",
    "   output_prefix = 'gs://{0}/javahelp/output'.format(BUCKET)\n",
    "   searchTerm = 'import'\n",
    "\n",
    "   # find all lines that contain the searchTerm\n",
    "   (p\n",
    "      | 'GetJava' >> beam.io.ReadFromText(input)\n",
    "      | 'Grep' >> beam.FlatMap(lambda line: my_grep(line, searchTerm) )\n",
    "      | 'write' >> beam.io.WriteToText(output_prefix)\n",
    "   )\n",
    "\n",
    "   p.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
