{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCP Professional Data Engineer\n",
    "### Leveraging Unstructured Data with Cloud Dataproc on Google Cloud Platform\n",
    "#### Modules:\n",
    "- Introduction to Cloud Dataproc\n",
    "- Running Dataproc Jobs\n",
    "- Leveraging GCP\n",
    "- Analyzing Unstructured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 1: Introduction to Cloud Dataproc\n",
    "### Introducing Cloud Dataproc\n",
    "- Cloud Dataproc is a fast, easy-to-use, fully-managed cloud service for running Apache Spark and Apache Hadoop clusters in a simpler, more cost-efficient way. Operations that used to take hours or days take seconds or minutes instead, and you pay only for the resources you use (with per-second billing). Cloud Dataproc also easily integrates with other Google Cloud Platform (GCP) services, giving you a powerful and complete platform for data processing, analytics and machine learning.\n",
    "\n",
    "### Why Unstructured Data\n",
    "- About 90% of enterprise data that is collected by a business tends to be unstructured. This includes:\n",
    "    - Emails, reviews, text, etc.\n",
    "- Consider the Google Steetview initiative- cars gathering photos at the street level with no immediate impact, yet now is the foundation for one of the most useful datasets available for autonomous cars.\n",
    "    \n",
    "### Why Cloud Dataproc\n",
    "- Horizontal vs vertical scaling\n",
    "- Hadoop origins:\n",
    "    - Based on a whitepaper describing MapReduce, Hadoop is an open-source distributed file system also known as HDFS. Spark is a framework that takes advantage of the distributed file system to effectively process tasks.\n",
    "- Running an onsite Hadoop cluster is costly and often inefficient.\n",
    "- Additional benefits:\n",
    "    - Stateless clusters in <90 seconds\n",
    "    - Supports Hadoop, Spark, Pig, Hive\n",
    "    - High-level APIs for job submission\n",
    "    - Connectors in BigTable,  BigQuery, Cloud Storage\n",
    "\n",
    "### Lab: Create a Dataproc Cluster\n",
    "\n",
    "#### Objectives:\n",
    "- Prepare a bucket for cluster initialization\n",
    "- Create a Dataproc Hadoop Cluster customized to use the Google Cloud API\n",
    "- Enable secure access to the Dataproc cluster\n",
    "- Explore Hadoop operations\n",
    "\n",
    "#### Task 1: Prepare Environment Variables\n",
    "- In the Console, on the Navigation menu () click Compute Engine > VM instances.\n",
    "- Locate the line with the instance called training_vm.\n",
    "- On the far right, under 'connect', Click on SSH to open a terminal window.\n",
    "- In this lab you will enter CLI commands on the training_vm.\n",
    "##### Create the source file for setting and resetting environment variables\n",
    "\n",
    "\n",
    "- In the training_vm SSH terminal window, using your preferred command line editor, create and edit the file to hold your environment variables. For example:\n",
    "- One environment variable that you will set is 'PROJECT_ID' that contains the Google Cloud project ID required to access billable resources.\n",
    "- In the Console, on the Navigation menu () click Home. In the panel with Project Info, the Project ID is listed. You can also find this information in the Qwiklabs tab under Connection Details, where it is labeled GCP Project ID.\n",
    "Add the environment variable to myenv for easy reference.\n",
    "- Dataproc can use a Cloud Storage bucket to stage its files during initialization. You can use this bucket to stage application programs or data for use by Dataproc. The bucket can also host Dataproc initialization scripts and output. The bucket name must be globally unique. Qwiklabs has already created a bucket for you that has the same name as the Project ID, which is already globally unique.\n",
    "- In the Console, on the Navigation menu () click Storage > Browser. Verify that the bucket exists. Notice the default storage class and the location (region) of this bucket. You will be using this region information next.\n",
    "Add the line to myenv to create an environment variable named \"BUCKET\".\n",
    "- You can use BUCKET in CLI commands. And if you need to enter the bucket name <your-bucket> in a text field in Console, you can quickly retrieve the name with echo BUCKET.\n",
    "- You will be creating a Dataproc cluster in a specific region. The Dataproc cluster and the bucket it will use for staging must be in the same region. Since the bucket you are using already exists, you will need to match the environment variable $MYREGION to the bucket region.\n",
    "- You can use find the region used by Qwiklabs on the Qwiklabs tab under Connection Details, labeled QL Region.\n",
    "The zone must be in the same region MYZONE will contain this value.\n",
    "- You can find the zone used by Qwiklabs on the Qwiklabs tab under Connection Details, labeled QL Zone.\n",
    "- Add the environment variables to myenv for easy reference.\n",
    "You will use the browser IP address to enable your local browser to reach the Dataproc cluster.\n",
    "- Find your computer's browser IP address by opening a browser window and viewing http://ip4.me/ Copy the IP address.\n",
    "Add the line to myenv to create an environment variable named BROWSER_IP.\n",
    "- After you have added all three definitions to myenv, and saved the file, use the source command to create the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-959bb4230150>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-959bb4230150>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    cd ~\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "cd ~ \n",
    "nano myenv\n",
    "PROJECT_ID=<project ID>\n",
    "BUCKET=<project ID>\n",
    "MYREGION=<region>\n",
    "MYZONE=<zone>\n",
    "BROWSER_IP=<your-browser-ip>\n",
    "\n",
    "# set environment variables\n",
    "source myenv\n",
    "# verify variables are set\n",
    "echo $PROJECT_ID\n",
    "echo $MYREGION $MYZONE\n",
    "echo $BUCKET\n",
    "echo $BROWSER_IP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2. Create a Dataproc Cluster\n",
    "- In the Console, on the Navigation menu () click Dataproc > Clusters.\n",
    "- Click Create Cluster.\n",
    "- Specify the following, and leave the remaining settings as their defaults:\n",
    "- Click on Preemptible workers, bucket, network, version, initialization, & access options\n",
    "- Specify the following, and leave the remaining settings as their defaults:\n",
    "    - Name: Cluster Dataproc\n",
    "    - Region: <myregion>\n",
    "    - Zone: <myzone>\n",
    "    - Cluster mode: Standard (1 master, n workers)\n",
    "    - (Master node) Machine type: n1-standard-2\n",
    "    - (Master node) Primary disk size: 100GB\n",
    "    - (Worker node) Machine type: n1-standard-1\n",
    "    - (Worker node) Primary disk size: 50GB\n",
    "    - Nodes: 3\n",
    "    - Network tags: hadoop access\n",
    "    - Cloud storage staging bucket: <your bucket>\n",
    "    - Image version: 1.2\n",
    "    - Project access: Allow API access\n",
    "    \n",
    "    \n",
    "- Create.\n",
    "- The cluster will take several minutes to become operational. In the Console, on the Navigation menu () click Dataproc > Clusters.\n",
    "- Click on your cluster, cluster-dataproc. Then click on the VM Instances tab. The instances will become operational before the hadoop software has completed initialization. When a checkmark in a green circle appears next to the name of the cluster, it is operational.\n",
    "\n",
    "#### Task 3: Enable secure access to Dataproc cluster\n",
    "- Create a firewall rule that allows access only to the Master Node from your computer's IP address. Only ports 8088 (Hadoop Job Interface) and 9870 (Hadoop Admin interface) will be permitted.\n",
    "- Port 8042 is the web UI for the node manager on the worker nodes and port 8080 is the default port for Datalab. Datalab is a notebook-based integrated development environment derived from Jupyter notebooks. It is a common tool for developing Dataproc applications. The Serverless Machine Learning on GCP course uses Datalab extensively.\n",
    "- Recall your computer's browser IP address for use in Console.\n",
    "\n",
    "    ```echo $BROWSER_IP```\n",
    "\n",
    "\n",
    "- In the Console, on the Navigation menu () click VPC Network > Firewall rules.\n",
    "- Click Create Firewall Rule.\n",
    "- Specify the following, and leave the remaining settings as their defaults:\n",
    "    - Name: allow-hadoop\n",
    "    - Network: default\n",
    "    - Priority: 1000\n",
    "    - Direction of traffic: Ingress\n",
    "    - Action on match: allow\n",
    "    - Targets: specified target tags\n",
    "    - Target tags: hadoopaccess\n",
    "    - Source IP ranges: <yourIP>32\n",
    "    - Specified ports and protocols tcp:9870;tcp:8088\n",
    "    \n",
    "- Verify that the network tag \"hadoopaccess\" is set on the Master Node. That will apply the firewall rule to the Master Node, giving your laptop access to it.\n",
    "- In the Console, on the Navigation menu () click Compute Engine > VM Instances.\n",
    "- Click on the Master Node, cluster-dataproc-m.\n",
    "- Verify that under Network Tags it lists hadoopaccess.\n",
    "- If the tag is not there, Click EDIT.\n",
    "- Under Network Tags add the tag: hadoopaccess\n",
    "- Click Save.\n",
    "\n",
    "#### Task 4: Explore Hadoop Operations\n",
    "- In the Console, on the Navigation menu () click Compute Engine > VM Instances.\n",
    "- In the list of VM instances, in the row for cluster-dataproc-m, highlight the External IP and copy it.\n",
    "- Open a new browser tab or window and paste the External IP. Add \":8088\" after the IP and press enter. Example: <External IP>:8088 The web page displayed is the Hadoop Applications interface.\n",
    "- Open a new browser tab or window. Paste the External IP. Add \":9870\" after the IP and press return. Example: <External IP>:9870 The webpage displayed is the Hadoop Administration Interface and should look something like this:\n",
    "- Click on the Datanodes tab. This will show you how much capacity is being used on the worker nodes HDFS (Hadoop Distributed File System) and how much capacity remains.\n",
    "- Click on Utilities > Logs. This shows you the Hadoop log files for each node in the cluster. This is where you can go to investigate problems with Hadoop. Use your browser's back button to return to the Hadoop Administration console.\n",
    "- Click on Utilities > Browse the file system. After a few moments the file system will appear in the browser page. You can use this to navigate the files system. In the row that says Owner is hdfs and Group is hadoop, click on the link that says user. -- - Here you can see directories for all the hadoop services.\n",
    "- Leave the JobTracker <External IP>:8088 and the Administration Interface <External IP>:9870 tabs or windows open. You will use them in the next task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 1 Review\n",
    "\n",
    "1.) Which of the following statements is true about Cloud Dataproc?\n",
    "- Lets you run Spark and Hadoop clusters with minimal administrations\n",
    "- Helps you create job-specific clusters w/o HDFS\n",
    "\n",
    "2.) Matching definitions:\n",
    "- Zone: determines the Google data center where compute nodes will be\n",
    "- Preemtible: costs less but may not always be available\n",
    "- Standard cluster mode: Provides 1 master and n workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 2: Running Dataproc Jobs\n",
    "### Running Jobs\n",
    "- Secure Shell (SSH) is a cryptographic network protocol for operating network services securely over an unsecured network.[1] The standard TCP port for SSH is 22. The best known example application is for remote login to computer systems by users.\n",
    "\n",
    "### Lab: Work with structured and semi-structured Data\n",
    "#### Objectives:\n",
    "- Use the Hive CLI and run a Pig job\n",
    "- Hive is used for structured data, similar to SQL\n",
    "- Pig is used for semi-structured data, similar to SQL + scripting\n",
    "\n",
    "Task 1: Preparation\n",
    "- A Dataproc cluster has been prepared for you. If you login to GCP before the progress bar reports that the \"Lab is Running\", you may have to wait several minutes for the cluster to transition from \"Provisioning\" to \"Running\" before the cluster completes setup.\n",
    "- You will be performing most of the lab steps from the Master Node of the cluster in an SSH terminal window.\n",
    "- In the Console, on the Navigation menu () click Dataproc > Clusters.\n",
    "- Locate the cluster named dataproc-cluster. Which region and zone is it located in? The region and zone have been selected automatically for you by Qwiklabs.\n",
    "- Notice the Cloud Storage staging bucket defined for this cluster. This bucket has the same name as the project ID, which is a convenient way to make the name globally unique.\n",
    "- Click on the name dataproc-cluster to go to the Cluster details page.\n",
    "- The Cluster details page opens to the \"Overview\" tab. Click on the tab labeled \"VM Instances\".\n",
    "- Open the Master Node terminal\n",
    "- On the line for the VM named dataproc-cluster-m you will see that it has the Role of Master and there is an SSH link next to it. Click on SSH to open a terminal window to the Master Node.\n",
    "\n",
    "Task 2. Enable secure web access to the Dataproc cluster\n",
    "- Create a restrictive firewall rule using Target tags, IP address, and protocol\n",
    "- Create a firewall rule that allows access only to the Master Node from your computer's IP address. Only ports 8088 (Hadoop Job Interface) and 9870 (Hadoop Admin interface) will be permitted.\n",
    "- Verify that the network tag is set on the Master Node\n",
    "- Verify that the network tag \"hadoopaccess\" is set on the Master Node. That will apply the firewall rule to the Master Node, giving your laptop access to it.\n",
    "- In the Console, on the Navigation menu () click Compute Engine > VM Instances.\n",
    "- Click on the Master Node, cluster-dataproc-m.\n",
    "- Verify that under Network Tags it lists hadoopaccess.\n",
    "- If the tag is not there, Click EDIT.\n",
    "- Under Network Tags add the tag: hadoopaccess\n",
    "- Click Save.\n",
    "- Identify the browser IP address\n",
    "- You will use the browser IP address to allow your local browser to connect to the Dataproc cluster.\n",
    "- Find your computer's browser IP address by opening a browser window and viewing http://ip4.me/ Copy the IP address.\n",
    "- Create the firewall rule\n",
    "- In the Console, on the Navigation menu () click VPC Network > Firewall rules.\n",
    "- Click Create Firewall Rule.\n",
    "- Specify the following, and leave the remaining settings as their defaults:\n",
    "    - Name: allow-hadoop\n",
    "    - Network: default\n",
    "    - Priority: 1000\n",
    "    - Direction of traffic: Ingress\n",
    "    - Action on match: allow\n",
    "    - Targets: specified target tags\n",
    "    - Target tags: hadoopaccess\n",
    "    - Source IP ranges:  |yourIP|/32\n",
    "    - Specified ports and protocols tcp:9870;tcp:8088\n",
    "- Click create.\n",
    "    \n",
    "Task 3. Prepare the data for Hive\n",
    "- Copy sample files to the Master node home directory\n",
    "- The sample files you need are have already been archived on the Master Node. You will need to copy them into your user directory with the following command.\n",
    "- In the Master Node SSH terminal window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-1ca6502de273>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-1ca6502de273>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    cp -r /training .\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "cd\n",
    "cp -r /training .\n",
    "ls\n",
    "cd training/training-data-analyst/courses/unstructured\n",
    "ls pet*.*\n",
    "# view structured data in text file  \n",
    "cat pet-details.txt\n",
    "# stage data in HDFS\n",
    "hadoop fs -mkdir /pet-details\n",
    "hadoop fs -put pet-details.txt /pet-details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the Console, on the Navigation menu () click Compute Engine > VM Instances.\n",
    "- In the list of VM instances, in the row for cluster-dataproc-m, highlight the External IP and copy it.\n",
    "- Open a new browser tab or window and paste the External IP. Add \":9870\" after the IP and press enter. Example: <External IP>:9870\n",
    "- You should now see the Hadoop Administration interface. Under Utilities, click on Browse the file system. Click on the folder /pet-details.\n",
    "- Notice that the file pet-details.txt is inside /pet-details.\n",
    "- Leave the Hadoop Administration interface open. You will return to it in later steps.\n",
    "    \n",
    "Task 4. Explore Hive using the Hive interactive CLI\n",
    "- Use HIVE to access the data in HDFS as if it were in a database\n",
    "- Hive provides a subset of SQL. The way it does this is by maintaining metadata to define a schema on top of the data. This is one way to work with a large amount of distributed data in HDFS using familiar SQL syntax.\n",
    "- In the master node SSH window, make sure you are in the right directory and start the Hive CLI interpreter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-4bc48b4f45bf>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-4bc48b4f45bf>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    CREATE DATABASE pets;\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "hive # initialize hive CLI interpreter\n",
    "CREATE DATABASE pets;\n",
    "USE pets\n",
    "# create table\n",
    "CREATE TABLE details (Type String, Name String, Breed String, Color String, Weight Int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\n",
    "SHOW TABLES;\n",
    "DESCRIBE pets.details;\n",
    "# establish relationship between metadata schema and data in HDFS\n",
    "load data INPATH '/pet-details/pet-details.txt' OVERWRITE INTO TABLE details;\n",
    "# verify that everything is working\n",
    "SELECT * FROM pets.details;\n",
    "# quit HIVE interpreter\n",
    "quit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the Hadoop Administration interface to see how hive works\n",
    "- Hive ingested the pet-details.txt file into a data warehouse format requiring a schema. You will use the Hadoop Administration interface to see this transformation.\n",
    "- Return to the Hadoop Administration interface in the browser.\n",
    "- Under Utilities, click on Browse the file system. Click on the folder /pet-details. The file pet-details.txt is gone.\n",
    "- Under Utilities, click on Browse the file system. Then click on user > hive > warehouse > pets.db > details. The file pet-details.txt has been moved to this location.\n",
    "\n",
    "***Note: Hive is designed for batch jobs and not for transactions. It ingests data into a data warehouse format requiring a schema. It does not support real-time queries, row-level updates, or unstructured data. Some queries may run much slower than others due to the underlying transformations Hive has to implement to simulate SQL.\n",
    "\n",
    "Task 5: Run a Pig job\n",
    "\n",
    "- In the master nodes SSH windowm view the Pig application:\n",
    "\n",
    "```cat pet-details.pig```\n",
    "\n",
    "- In line 'x1', the load statement in the application creates a schema on top of the HDFS data file. Lines 'x2' through 'x5' perform transformations on the data. And the last line stores the result in a folder called /GroupedByType in HDFS.\n",
    "- The application expects to find the ingest file in HDFS in the directory /pet-details. Make another copy of the data at that location:\n",
    "\n",
    "```hadoop fs -put pet-details.txt /pet-details```\n",
    "\n",
    "Run the application:\n",
    "\n",
    "```pig < pet-details.pig```\n",
    "\n",
    "- Return to the browser tab containing the Hadoop Applications interface and refresh it, or reopen it with <External-IP>:8088. Notice that Pig generated a Java MapReduce job which is running on the cluster. Click the browser refresh button to watch for job completion.\n",
    "- Return to the browser tab containing the Hadoop Administration interface and refresh it, or reopen it with <External-IP>:9870. Under Utilities, click on Browse the file system. In the resulting list, click on GroupedByType. This is the output directory specified in the Pig application. The file named part-r-00000 is the HDFS file containing the output. You cannot view the contents from here. First, you must download that part to the local file system\n",
    "- Return to the SSH terminal on the Master node, cloud-dataproc-m and make a local output directory and retrieve the results from HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-cb82c8aaa442>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-cb82c8aaa442>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    mkdir output\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "cd\n",
    "mkdir output\n",
    "cd output\n",
    "hadoop fs -get /GroupedByType/part* .\n",
    "# view results of the pig job\n",
    "cat part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note: Pig provides SQL primitives similar to Hive, but in a more flexible scripting language format. Pig can also deal with semi-structured data, such as data having partial schemas, or for which the schema is not yet known. For this reason it is sometimes used for Extract Transform Load (ETL). It generates Java MapReduce jobs. Pig is not designed to deal with unstructured data.\n",
    "\n",
    "### End Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separation of Storage & Compute\n",
    "### Submitting Jobs\n",
    "### Spark RDDs, Transformations, and Actions\n",
    "### Lab: Working with Spark Jobs\n",
    "### Module 2 Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 3: Leveraging GCP\n",
    "### Big Query Support\n",
    "### Lab: Leverage GCP\n",
    "#### Objectives:\n",
    "- Explore Spark using PySpark jobs\n",
    "- Using Cloud Storage instead of HDFS\n",
    "- Run a PySpark application from Cloud Storage\n",
    "- Using Python Pandas to add BigQuery to a Spark application\n",
    "\n",
    "Task 1: Prepare the Master Node and the Bucket\n",
    "- In the Console, on the Navigation menu () click Dataproc > Clusters.\n",
    "- Locate the cluster named dataproc-cluster.\n",
    "- Click on the name dataproc-cluster to go to the Cluster details page.\n",
    "- The Cluster details page opens to the \"Overview\" tab. Click on the tab labeled \"VM Instances\".\n",
    "- On the line for the VM named dataproc-cluster-m you will see that it has the Role of Master and there is an SSH link next to it. Click on SSH to open a terminal window to the Master Node.\n",
    "- In the Master Node SSH terminal window, type:\n",
    "\n",
    "```\n",
    "cd\n",
    "cp -r /training .\n",
    "ls\n",
    "```\n",
    "\n",
    "#### Note: A Cloud Storage bucket has already been created for you. It has the same name as the Project ID. You will create an environment variable to make it easy to reference the bucket from the command line on the Master Node.\n",
    "\n",
    "- In the console, on the Navigation menu click Storage -> Bucket\n",
    "- In the Master Node SSH terminal window:\n",
    "\n",
    "```\n",
    "BUCKET=<bucket-name>\n",
    "echo $BUCKET\n",
    "```\n",
    "\n",
    "Task 2: The two letter lab\n",
    "\n",
    "#### Note: Why would you want to use Cloud Storage instead of HDFS?\n",
    "You can shut down the cluster when you are not running jobs. The storage persists even when the cluster is shut down, so you don't have to pay for the cluster just to maintain data in HDFS.\n",
    "In some cases Cloud Storage provides better performance than HDFS.\n",
    "Cloud Storage does not require the administration overhead of a local file system\n",
    "\n",
    "- Place a copy of your sample data file in a Cloud Storage bucket instead of HDFS.\n",
    "- In the Master Node terminal window, enter the following gsutil command to copy the sample text files to the Cloud Storage bucket.\n",
    "\n",
    "```\n",
    "gsutil cp /training/road-not-taken.txt gs://$BUCKET\n",
    "```\n",
    "\n",
    "- In the SSH terminal for the Master Node, use nano or vi to create the file wordcount.py\n",
    "- Copy and paste the following code into the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f72fd94c7f86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Okay Google.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "import re\n",
    "\n",
    "print(\"Okay Google.\")\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"CountUniqueWords\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "lines = spark.read.text(\"/sampledata/road-not-taken.txt\").rdd.map(lambda x: x[0])\n",
    "counts = lines.flatMap(lambda x: x.split(' ')) \\\n",
    "                  .filter(lambda x: re.sub('[^a-zA-Z]+', '', x)) \\\n",
    "                  .filter(lambda x: len(x)>1 ) \\\n",
    "                  .map(lambda x: x.upper()) \\\n",
    "                  .map(lambda x: (x, 1)) \\\n",
    "                  .reduceByKey(add) \\\n",
    "                  .sortByKey()\n",
    "output = counts.collect()\n",
    "for (word, count) in output:\n",
    "  print(\"%s = %i\" % (word, count))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, verify that the data file does not exist in HDFS:\n",
    "```\n",
    "hadoop fs -ls\n",
    "```\n",
    "- Next, use the Hadoop file system command to view the files through the hadoop connector to Cloud Storage. This verifies that the connector is working and that the file is available in the bucket.\n",
    "```\n",
    "hadoop fs -ls gs://$BUCKET\n",
    "```\n",
    "- Edit wordcount.py in nano or vi\n",
    "\n",
    "```\n",
    "lines = spark.read.text(\"/sampledata/road-not-taken.txt\").rdd.map(lambda x: x[0])\n",
    "```\n",
    "\n",
    "- With a line the refers to the file in Cloud Storage. Remember to remove \"/sampledata\" because that directory does not exist. Remember to use the actual bucket name and not the environment variable. The Worker Nodes on the cluster where the program will run do not know the value of the local environment variable on the Master Node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = spark.read.text(\"gs://<YOUR-BUCKET>/road-not-taken.txt\").rdd.map(lambda x: x[0])\n",
    "# run the job\n",
    "spark-submit wordcount.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Run a Pyspark application from Cloud Storage\n",
    "\n",
    "- In the previous task you created a PySpark application in a development environment (on the Master Node). You tested the application using spark-submit.\n",
    "- In this task you will migrate the application from the development environment to a production environment. You will stage the working application file in Cloud Storage. And you will run the production job from Console.\n",
    "- In the Master Node terminal, use the following command to copy the tested wordcount.py PySpark application to the bucket.\n",
    "```\n",
    "gsutil cp wordcount.py gs://$BUCKET\n",
    "```\n",
    "\n",
    "- In the Console, on the Navigation menu () click Dataproc > Clusters. Take note of the region where the cluster is located. You will need that in the next steps.\n",
    "- You will also need the bucket name. You can also retrieve the bucket name from the Master Node terminal by entering the following. Highlight the bucket name and copy it.\n",
    "\n",
    "```\n",
    "echo $BUCKET\n",
    "```\n",
    "- In the Console, on the Navigation menu () click Dataproc > Jobs.\n",
    "- Submit job and specifcy the following:\n",
    "    - Region: <your-region>\n",
    "    - Cluster: dataproc-cluster\n",
    "    - Job type: PySpark\n",
    "    - Main python file: gs://<your bucket>/wordcount.py\n",
    "- Submit and end. Check Dataproc -> Jobs for progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing Clusters\n",
    "### Lab: Cluster Automation using CLI Commands\n",
    "### Module 3 Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4: Analyzing Unstructured Data\n",
    "### Infuse Your Business With Machine Learning\n",
    "### Lab: Add Machine Learning\n",
    "### Module 4 Review"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
