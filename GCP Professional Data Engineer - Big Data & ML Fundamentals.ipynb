{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCP Professional Data Engineer\n",
    "### Introduction to the Data and Machine Learning on Google Clouud Platform Specialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 1: Introduction to Google Cloud Platform and its Big Data Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#! What is the Google Cloud Platform?\n",
    "\n",
    "GCP is the natural evolution from personal compute centers to a global, distributed computational network.\n",
    "\n",
    "### Data technologies to reseach\n",
    "2002 - GFS\n",
    "2004 - MapReduce (deprecated)\n",
    "2006 - BigTable\n",
    "2008 - Dremel (dataprocessing)\n",
    "2009 - Colossus\n",
    "2010 - Flume (dataprocessing)\n",
    "2011 - Megastore\n",
    "2012 - Spanner\n",
    "2013 - Millwheel\n",
    "2014 - PubSub / F1\n",
    "2015 - TensorFlow\n",
    "\n",
    "http://research.google.com/pubs/papers.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#! GCP Big Data Products\n",
    "\n",
    "### A functional view of the platform:\n",
    "\n",
    "Foundation - Compute engine, Cloud storage\n",
    "Databases - Datastore, Cloud SQL, Cloud Bigtable\n",
    "Analytics & ML - BigQuery, CloudDatalab, Translate API\n",
    "Data-handling Frameworks - Cloud PubSub, Cloud Dataflow, Cloud Dataproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#!! Module 1 Review \n",
    "\n",
    "Q. What is success for you?\n",
    "A. ~Confidential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#! Welcome to the Foundations of GCP Compute and Storage\n",
    "\n",
    "CPUs on Demand\n",
    "https://cloud.google.com/custom-machine-types\n",
    "https://cloud.google.com/compute/pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab: Create a Compute Engine Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "- Create a Compute Engine instance with the necessary Access and Security\n",
    "- SSH into the instance\n",
    "- Install the software package Git (for source code version control)\n",
    "\n",
    "Launch GCP via console.cloud.google.com\n",
    "\n",
    "Task 1: Create Compute Engine instance with the necessary API access\n",
    "\n",
    "- In the GCP Console, on the Navigation menu (8ab244f9cffa6198.png), click Compute Engine.\n",
    "- Click Create and wait for a form to load. You will need to change some options on the form that comes up.\n",
    "- For Name, leave the default value, for Region, select us-central1, and for Zone, select us-central1-a.\n",
    "- For Identify and API access, in Access scopes, select Allow full access to all Cloud APIs:\n",
    "\n",
    "Task 2: SSH into the Instance\n",
    "\n",
    "- When the instance you just created is available, click SSH\n",
    "- To view information about the Compute Engine instance you just launched, type the following into your SSH terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Install Software:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Install git\n",
    "sudo apt-get update\n",
    "sudo apt-get -y -qq install git\n",
    "# Verify installation\n",
    "git --version\n",
    "# exit\n",
    "exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Lab 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 2: Foundations of GCP Compute and Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#! A Global Filesystem\n",
    "\n",
    "https://cloud.google.com/storage/docs/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab: Interact With Cloud Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives: \n",
    "- Create a Compute Engine instance with the necessary Access and Security\n",
    "- SSH into the instance\n",
    "- Install the software package Git (for source code version control)\n",
    "- Ingest data into a Compute Engine instance\n",
    "- Transform data on the Compute Engine instance\n",
    "- Store the transformed data on Cloud Storage\n",
    "- Publish Cloud Storage data to the web\n",
    "\n",
    "###### Sample code for transformation\n",
    "https://github.com/GoogleCloudPlatform/datalab-samples/blob/master/basemap/earthquakes.ipynb\n",
    "\n",
    "\n",
    "Task 1: Create Compute Engine instance with the necessary API access\n",
    "\n",
    "Ibid.\n",
    "\n",
    "Task 2: SSH into the instance\n",
    "\n",
    "Ibid.\n",
    "\n",
    "Task 3: Install software and Ingest USGS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Install git\n",
    "sudo apt-get update\n",
    "sudo apt-get -y -qq install git\n",
    "# Verify installation\n",
    "git --version\n",
    "# Copy data from Github and navigate to folder\n",
    "git clone https://github.com/GoogleCloudPlatform/training-data-analyst\n",
    "cd training-data-analyst/CPB100/lab2b\n",
    "# Examine \n",
    "less ingest.sh # less allows you to view the file in the terminal\n",
    "bash ingest.sh # bash runs the file\n",
    "head earthquakes.csv # head show the first few lines\n",
    "# Install python packages\n",
    "bash install_missing.sh\n",
    "# Run transform code from link in lab description\n",
    "python transform.py\n",
    "# List directory comments\n",
    "ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5: Create bucket\n",
    "\n",
    "- In the GCP Console, on the Navigation menu, click Storage.\n",
    "- Click Create Bucket.\n",
    "- For Name, enter your Project ID, then click Create. To find your Project ID, click the project in the top menu of the GCP Console and copy the value under ID for your selected project.\n",
    "\n",
    "Task 6: Store Data\n",
    "\n",
    "- In your SSH terminal, type the following, replacing <YOUR-BUCKET> with the name of the bucket you created in the previous task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsutil cp earthquakes.* gs://<YOUR-BUCKET>/earthquakes/ # move data to bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the GCP Console, click the bucket name and notice there are three new files present in the earthquakes folder (click Refresh if necessary).\n",
    "\n",
    "Task 7: Publish Cloud Storage files to web\n",
    "\n",
    "- In the GCP Console, check public link for all the three files in the earthquake folder.\n",
    "- For earthquakes.htm, click Public link.\n",
    "\n",
    "##### Note: this feature was non-obvious, possibly due to a UI change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Lab 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#! Module 2 Review\n",
    "\n",
    "1. Compute nodes on GCP are:\n",
    "- Allocated on demand, and you pay for the time they are up\n",
    "- Cheaper if you allow them to be shutdown at anytime\n",
    "\n",
    "2. Google Cloud Storage is a good option for storing data that:\n",
    "- May be required to be read at some later time\n",
    "- May be imported into a cluster for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 3: Data Analysis on the Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#! Resources\n",
    "\n",
    "Compute Engine: https://cloud.google.com/compute/\n",
    "Storage: https://cloud.google.com/storage/\n",
    "Pricing: https://cloud.google.com/pricing/\n",
    "Cloud Launcher: https://cloud.google.com/launcher/\n",
    "Pricing Philosophy: https://cloud.google.com/pricing/philosophy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab: Working with Cloud SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "- Create Cloud SQL instance\n",
    "- Create database tables by importing .sql files from Cloud Storage\n",
    "- Populate the tables by importing .csv files from Cloud Storage\n",
    "- Allow access to Cloud SQL\n",
    "- Explore the rentals data using SQL statements from CloudShell\n",
    "\n",
    "Task 1: Access lab code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "git clone https://github.com/GoogleCloudPlatform/training-data-analyst\n",
    "# Navigate to folder\n",
    "cd training-data-analyst/CPB100/lab3a\n",
    "# Examine file\n",
    "less cloudsql/table_creation.sql\n",
    "# Examine first few rows\n",
    "head cloudsql/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Create bucket\n",
    "- In the GCP Console, on the Navigation menu.\n",
    "- Click Storage.\n",
    "- Click Create Bucket.\n",
    "- For Name, enter your Project ID, then click Create. To find your Project ID, click the project in the top menu of the GCP Console and copy the value under ID for your selected project.\n",
    "\n",
    "Task 3: Stage .sql & .csv files into Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# move files to bucket\n",
    "gsutil cp cloudsql/* gs://<BUCKET-NAME>/sql/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4:  Create Cloud SQL Instance\n",
    "- In the GCP console, click SQL (in the Storage section).\n",
    "- Click Create instance.\n",
    "- Click Choose MySQL, then click Configure MySQL Development.\n",
    "- For Instance ID, type rentals\n",
    "- Specify password\n",
    "- Scroll down and click Show configuration options. Click Authorize networks, then click + Add network.\n",
    "- From Cloud Shell within the lab3a directory, find your IP address by typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find IP address\n",
    "bash ./find_my_ip.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the New network dialog, enter any Name, and for Network, type the IP address from the previous step. Click Done.\n",
    "\n",
    "Note: If you lose your Cloud Shell VM due to inactivity, you will have to reauthorize your new Cloud Shell VM with Cloud SQL. For your convenience, lab3a includes a script called authorize\\_cloudshell.sh that you can run.\n",
    "\n",
    "- Click Create to create the instance. It will take a minute or so for your Cloud SQL instance to be provisioned.\n",
    "- Note down the IP address of your Cloud SQL instance (from the browser window) in the third row of the table you started.\n",
    "\n",
    "Task 5: Create tables\n",
    "\n",
    "- In Cloud SQL, click rentals to view instance information.\n",
    "- Click Import(on the top menu bar).\n",
    "- Click Browse. This will bring up a list of buckets. Click on the bucket you created, then navigate into sql and click table_creation.sql.\n",
    "- Click Select, then click Import. \n",
    "\n",
    "Task 6: Populate tables\n",
    "\n",
    "- To import CSV files from Cloud Storage, from the GCP console page with the Cloud SQL instance details, click Import (top menu).\n",
    "- Click Browse, browse in the bucket you created to sql, then click accommodation.csv. Click Select.\n",
    "- For Database, select recommendation_spark.\n",
    "- For Table, type Accommodation.\n",
    "- Click Import.\n",
    "- Repeat the Import (steps 1 - 5) for rating.csv, but for Table, type Rating.\n",
    "\n",
    "Task 7: Explore Cloud SQL\n",
    "\n",
    "- To explore Cloud SQL, you can use the mysql CLI. In Cloud Shell, type the following, replacing MYSQLIP with the Public IP address of your rentals instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mysql --host=<MySQLIP> --user=root --password"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The IP address is the one for the database server (i.e. the third row in the notes). You can also find it on the instance details on the cloud console.\n",
    "- MySQL will prompt you for the root password. Type that into the prompt when prompted.\n",
    "- In Cloud Shell, at the mysql prompt, type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use recommendation_spark; # sets database in mysql session\n",
    "show tables; # view list of tables\n",
    "select * from Rating; # verify data was loaded\n",
    "select * from Accommodation where type = 'castle' and price < 1500; # check for cheap castles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Lab 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab: Recommendations ML w/ Dataproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "    \n",
    "- Launch Dataproc\n",
    "- Train and apply ML model written in PySpark to create product recommendations\n",
    "- Explore inserted rows in Cloud SQL \n",
    "\n",
    "Task 1: Create Assets\n",
    "\n",
    "- In Cloud Shell, clone the repo using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "git clone https://github.com/GoogleCloudPlatform/training-data-analyst\n",
    "# Navigate to folder containing training data\n",
    "cd training-data-analyst/CPB100/lab3a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the GCP Console, on the Navigation menu (8ab244f9cffa6198.png), click Storage.\n",
    "- Click Create Bucket.\n",
    "- For Name, enter your Project ID, then click Create. To find your Project ID, click the project in the top menu of the GCP Console and copy the value under ID for your selected project.\n",
    "- Finally, stage the table definition and data files into Cloud Storage, so that you can later import them into Cloud SQL from Cloud Shell within the lab3a directory by typing the following, replacing <BUCKET-NAME> with the name of the bucket you just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsutil cp cloudsql/* gs://<BUCKET-NAME>/sql/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the GCP console, go to Storage, navigate to your bucket and verify that the .sql and .csv files now exist on Cloud Storage.\n",
    "\n",
    "Task 2: Create Cloud SQL instance\n",
    "\n",
    "- In the GCP Console, on the Navigation menu (8ab244f9cffa6198.png), click SQL (in the Storage section).\n",
    "- Click Create Instance.\n",
    "- Click Choose MySQL, then click Configure MySQL Development.\n",
    "- For Instance ID, type rentals.\n",
    "- Scroll down and specify a root password. Before you forget, note down the root password (please don't do this in real-life!).\n",
    "- Scroll down and in Configuration options, click Authorize networks - Add network.\n",
    "- In Cloud Shell, make sure you're in the lab3a directory and find your IP address by typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bash ./find_my_ip.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the Add Network dialog, enter an optional Name and enter the IP address output in the previous step. Click Done.\n",
    "\n",
    "Note: If you lose your Cloud Shell VM due to inactivity, you will have to reauthorize your new Cloud Shell VM with Cloud SQL. For your convenience, lab3a includes a script called authorize\\_cloudshell.sh that you can run.\n",
    "\n",
    "- Click Create to create the instance. It will take a minute or so for your Cloud SQL instance to be provisioned.\n",
    "- Note down the Public IP address of your Cloud SQL instance (from the browser window).\n",
    "\n",
    "Task 3: Create and populate tables\n",
    "\n",
    "- Click rentals to view details about your Cloud SQL instance.\n",
    "- Click Import.\n",
    "- Click Browse. This will bring up a list of buckets. Click on the bucket you created, then navigate into /sql, click table_creation.sql, then click Select.\n",
    "- Click Import.\n",
    "- Next, to import CSV files from Cloud Storage, click Import.\n",
    "- Click Browse, navigate into /sql, click accommodation.csv, then click Select.\n",
    "- Fill out the rest of the dialog as follows:\n",
    "- For Database, select recommendation_spark\n",
    "- For Table, type Accommodation\n",
    "- Click Import.\n",
    "- Repeat the Import process (steps 5 - 8) for rating.csv, but for Table, type Rating\n",
    "\n",
    "Task 4: Launch Dataproc\n",
    "\n",
    "- In the GCP Console, on the Navigation menu, click SQL and note the region of your Cloud SQL instance:\n",
    "- In the GCP Console, on the Navigation menu, click Dataproc and click Enable API if prompted. Once enabled, click Create cluster.\n",
    "- Change the zone to be in the same region as your Cloud SQL instance. This will minimize network latency between the cluster and the database.\n",
    "- For Master node, for Machine type, select 2 vCPU (n1-standard-2).\n",
    "- For Worker nodes, for Machine type, select 2 vCPU (n1-standard-2).\n",
    "- Leave all other values with their default and click Create. It will take 1-2 minutes to provision your cluster.\n",
    "- Note the Name, Zone and Total worker nodes in your cluster.\n",
    "- In Cloud Shell, navigate to the folder corresponding to this lab and authorize all the Dataproc nodes to be able to access your Cloud SQL instance, replacing <Cluster-Name>, <Zone>, and <Total-Worker-Nodes> with the values you noted in the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd ~/training-data-analyst/CPB100/lab3b\n",
    "bash authorize\n",
    "ataproc.sh <Cluster-Name> <Zone> <Total-Worker-Nodes>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5: Run ML model\n",
    "\n",
    "- Edit the model training file using nano:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nano sparkml/train_and_apply.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change the fields marked #CHANGE at the top of the file (scroll down using the down arrow key) to match your Cloud SQL setup (see earlier parts of this lab where you noted these down), and save the file using Ctrl+O then press Enter, and then press Ctrl+X to exit from the file.\n",
    "- Copy this file to your Cloud Storage bucket using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsutil cp sparkml/tr*.py gs://<bucket-name>/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the Dataproc console, click Jobs.\n",
    "- Click Submit job.\n",
    "- For Job type, select PySpark and for Main python file, specify the location of the Python file you uploaded to your bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs://<bucket-name>/train_and_apply.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Click Submit and wait for the job Status to change from Running (this will take up to 5 minutes) to Succeeded.\n",
    "\n",
    "Task 6: Explore inserted rows\n",
    "\n",
    "- In Cloud Shell, authorize your CloudShell VM to access the Cloud SQL instance. This will also deauthorize the Dataproc cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bash ../lab3a/authorize_cloudshell.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Connect to your Cloud SQL instance, replacing <MySQLIP> with your SQL instance Public IP Address noted in an earlier task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mysql --host=<MySQLIP> --user=root --password"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the mysql prompt, type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use recommendation_spark; # set database in mysql session\n",
    "# find recommendations for some user\n",
    "select r.userid, \n",
    "r.accoid, \n",
    "r.prediction, \n",
    "a.title, \n",
    "a.location, \n",
    "a.price, \n",
    "a.rooms, \n",
    "a.rating, \n",
    "a.type \n",
    "from Recommendation as r, \n",
    "Accommodation as a \n",
    "where r.accoid = a.id and r.userid = 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Lab 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#! Module 3 Review\n",
    "\n",
    "1. Relational databases are a good choice when you need:\n",
    "- Transactional updates on relatively small datasets\n",
    "\n",
    "2. Cloud SQL and Cloud Dataproc offer familiar tools (MySQL and Hadoop/Pig/Hive/Spark). What is the value-add provided by Google Cloud Platform?\n",
    "- Google-proprietary extensions and bug fixes to MySQL, Hadoop, and so on\n",
    "- Fully-managed versions of the software offer no-ops\n",
    "- Running it on Google infrastructure offers reliability and cost savings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4: Scaling Data Analysis: Compute with GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "\n",
    "- Employ BigQuery and Cloud Datalab to carry out interactive data analysis\n",
    "- Train and use a neural network using TensorFlow\n",
    "\n",
    "#### Sections:\n",
    "\n",
    "#! Intro to Scaling Data Analysis: Change How You Compute w/ GCP\n",
    "#! Fast Random Access\n",
    "#! Interactive, iterative development\n",
    "#! Warehouse and query petabytes\n",
    "#! Machine learning w/ TensorFlow\n",
    "#! Fully build machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab: Create ML Dataset with BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "\n",
    "- Use BigQuery and Datalab to explore and visualize data\n",
    "- Build a Pandas dataframe that will be used as the training dataset for machine learning using TensorFlow\n",
    "\n",
    "Task 1: Launch Cloud Datalab\n",
    "\n",
    "- In Cloud Shell, type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gcloud compute zones list\n",
    "datalab create bdmlvm --zone <ZONE> # choose a zone from list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Checkout notebook into Cloud Datalab\n",
    "\n",
    "- Click on the Web Preview icon (looks like a browser <> window) on the top-right corner of the Cloud Shell ribbon. Click on Change port. Switch to port 8081 using the Change Preview Port dialog box, and then click on Change and Preview.\n",
    "\n",
    "Note: The connection to your Datalab instance remains open for as long as the datalab command is active. If the cloud shell used for running the datalab command is closed or interrupted, the connection to your Cloud Datalab VM will terminate. If that happens, you may be able to reconnect using the command datalab connect bdmlvm in your new Cloud Shell.\n",
    "\n",
    "- In Datalab, click on the icon for Open ungit in the top-right ribbon. (looks like a forked branch)\n",
    "- In the Ungit window, select the text that reads /content/datalab/notebooks and remove the notebooks so that it reads /content/datalab, then hit Enter.\n",
    "- In the panel that comes up, type the following as the GitHub repository to Clone from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://github.com/GoogleCloudPlatform/training-data-analyst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Open a Datalab notebook\n",
    "    \n",
    "- In the Datalab browser, navigate to training-data-analyst > CPB100 > lab4a > demandforecast.ipynb.\n",
    "- Read the commentary, Click Clear | Clear all Cells, then run the Python snippets (Use Shift+Enter to run each piece of code) in the cell, step by step.\n",
    "- When you reach the section Machine Learning with Tensorflow, please stop -- that is the next lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Lab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
