{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges associated with streaming data\n",
    "\n",
    "#### Ingesting variable volumes\n",
    "- massive amounts of streaming events, handle spiky/bursting data, high availability and durability\n",
    "- Cloud Pub/Sub (Ingest)\n",
    "\n",
    "#### Late data, unordered data\n",
    "- how to deal with latency, late arriving records, or speculative results\n",
    "- Data Dataflow (Processing & Imperative Analysis)\n",
    "\n",
    "#### Real-time insights\n",
    "- continuous query processing, visualization, analytics, etc.\n",
    "- Google BigQuery (Durable storage and interactive analysis)\n",
    "\n",
    "## Module 1 Review\n",
    "\n",
    "1.) Dataflow offers the following that makes it easy to create resilient streaming pipelines when working with unbounded data\n",
    "- Ability to flexibly reason about time\n",
    "- Control messages to ensure correctness\n",
    "\n",
    "2.) Match the GCP product with its role when designing streaming systems\n",
    "- Pub / Sub: Global messaging queue\n",
    "- Dataflow: Controls to handle late-arriving and out-of-order data\n",
    "- Bigtable: latency in the order of milliseconds when querying against overwhelming volume\n",
    "- BiqQuery: Query data as it arrives from streaming pipelines\n",
    "\n",
    "## Lab: Publish Streaming Data into Pub/Sub\n",
    "#### Objectives:\n",
    "- Create a Pub/Sub topic and subscription\n",
    "- Simulate your traffic sensor data into Pub/Sub\n",
    "\n",
    "#### Task 1: Preparation\n",
    "- In the Console, on the Navigation menu () click Compute Engine > VM instances.\n",
    "- Locate the line with the instance called training_vm.\n",
    "- On the far right, under 'connect', Click on SSH to open a terminal window.\n",
    "- In this lab you will enter CLI commands on the training_vm.\n",
    "- The training_vm is installing software in the background. Verify that setup is complete by checking that the following directory exists. If it does not exist, wait a few minutes and try again\n",
    "- A repository has been downloaded to the VM. Copy the repository to your home directory.\n",
    "\n",
    "```\n",
    "ls /training\n",
    "# copy to home directory\n",
    "cp -r /training/training-data-analyst/ .\n",
    "```\n",
    "- On the training_vm SSH terminal, set the DEVSHELL_PROJECT_ID environment variable and export it so it will be available to other shells.\n",
    "```\n",
    "export DEVSHELL_PROJECT_ID=<project-id>\n",
    "```\n",
    "\n",
    "#### Task 2: Create Pub/Sub topic and subscription\n",
    "- On the training_vm SSH terminal, navigate to the directory for this lab.\n",
    "```\n",
    "cd ~/training-data-analyst/courses/streaming/publish\n",
    "gcloud pubsub topics create sandiego\n",
    "gcloud pubsub topics publish sandiego --message \"hello\"\n",
    "gcloud pubsub subscriptions create --topic sandiego mySub1\n",
    "gcloud pubsub subscriptions pull --auto-ack mySub1\n",
    "# try again\n",
    "gcloud pubsub topics publish sandiego --message \"hello again\"\n",
    "gcloud pubsub subscriptions pull --auto-ack mySub1\n",
    "```\n",
    "\n",
    "- Return to the Console tab. On the Navigation menu () click Pub/Sub > Topics.\n",
    "- You should see a line with the Topic Name ending in sandiego and the number of Subscriptions set to 1.\n",
    "- In the training_vm SSH terminal,, cancel your subscription.\n",
    "\n",
    "```\n",
    "gcloud pubsub subscriptions delete mySub1\n",
    "```\n",
    "\n",
    "#### Task 3: Simulate traffic sensor data into Pub/Sub\n",
    "- Explore the python script to simulate San Diego traffic sensor data. Do not make any changes to the code.\n",
    "```\n",
    "cd ~/training-data-analyst/courses/streaming/publish\n",
    "nano send_sensor_data.py\n",
    "```\n",
    "- Download the traffic simulation dataset.\n",
    "```\n",
    "./download_data.sh\n",
    "sudo apt-get install -y python-pip\n",
    "sudo pip install -U google-cloud-pubsub\n",
    "./send_sensor_data.py --speedFactor=60 --project (dollar sign)DEVSHELL_PROJECT_ID\n",
    "```\n",
    "\n",
    "- This command simulates sensor data by sending recorded sensor data via Pub/Sub messages. The script extracts the original time of the sensor data and pauses between sending each message to simulate realistic timing of the sensor data. The value speedFactor changes the time between messages proportionally. So a speedFactor of 60 means '60 times faster' than the recorded timing. It will send about an hour of data every 60 seconds.\n",
    "\n",
    "#### Task 4: Verify that messages are received\n",
    "- In the Console, on the Navigation menu () click Compute Engine > VM instances.\n",
    "- Locate the line with the instance called training_vm.\n",
    "- On the far right, under 'connect', Click on SSH to open a second terminal window.\n",
    "- Change into the directory you were working in:\n",
    "```\n",
    "cd ~/training-data-analyst/courses/streaming/publish\n",
    "gcloud pubsub subscriptions create --topic sandiego mySub2\n",
    "gcloud pubsub subscriptions pull --auto-ack mySub2\n",
    "# cancel subscription\n",
    "gcloud pubsub subscriptions delete mySub2\n",
    "exit\n",
    "```\n",
    "\n",
    "## End Lab\n",
    "\n",
    "## Module 2 Review\n",
    "\n",
    "1.) Which of the following about Cloud Pub/Sub is NOT true?\n",
    "- Pub/Sub stores your messages indefinitely until you need it\n",
    "\n",
    "Pub/Sub does:\n",
    "- Simplify systems by removing the need for every component to speak to every component\n",
    "- Connect applications and devices through a messaging infrastructure\n",
    "\n",
    "2.) Cloud Pub/Sub guarantees that messages delivered are in the order they were received\n",
    "- False\n",
    "(Pub/Sub takes advantage of timestamping to deliver in the correct order)\n",
    "\n",
    "3.) Which of the following about Cloud Pub/Sub topics and subscriptions are true?\n",
    "- 1 or more publishers can write to the same topic\n",
    "- 1 or more subscribers can request from the same subscription\n",
    "\n",
    "4.) Which of the following delivery methods is ideal for subscribers needing close to real time performance?\n",
    "- Push delivery \n",
    "\n",
    "## Lab: Streaming Data Pipelines\n",
    "#### Objectives:\n",
    "- Launch Dataflow and run a Dataflow job\n",
    "- Understand how data elements flow through the transformations of a Dataflow pipeline\n",
    "- Connect Dataflow to Pub/Sub and BigQuery\n",
    "- Observe and understand how Dataflow autoscaling adjusts compute resources to process input data optimally\n",
    "- Learn where to find logging information created by Dataflow\n",
    "- Explore metrics and create alerts and dashboards with Stackdriver Monitoring\n",
    "\n",
    "#### Task 1: Preparation\n",
    "- In the Console, on the Navigation menu () click Compute Engine > VM instances.\n",
    "- Locate the line with the instance called training_vm.\n",
    "- On the far right, under 'connect', Click on SSH to open a terminal window.\n",
    "- In this lab you will enter CLI commands on the training_vm.\n",
    "```\n",
    "ls /training\n",
    "cp -r /training/training-data-analyst/ .\n",
    "source /training/project_env.sh\n",
    "```\n",
    "\n",
    "#### Task 2: Create a BigQuery Dataset and Cloud Storage Bucket\n",
    "- Open the BigQuery web UI. On the Navigation menu () click BigQuery.\n",
    "- In the left column, beneath the text box, find your project name. To the right of the project name, click the blue arrow. Choose Create new dataset.\n",
    "- In the â€˜Create Dataset' dialog, for Dataset ID, type demos and click OK.\n",
    "- In the Console, on the Navigation menu () click Storage > Browser.\n",
    "\n",
    "#### Task 3: Simulate traffic sensor data into Pub/Sub\n",
    "- In the training_vm SSH terminal, start the sensor simulator. The script reads sample data from a csv file and publishes it to Pub/Sub.\n",
    "```\n",
    "/training/sensor_magic.sh\n",
    "```\n",
    "- In the Console, on the Navigation menu () click Pub/Sub>Topics\n",
    "- Examine the line for Topic name for the topic sandiego. Notice that Subscriptions are currently at 0.\n",
    "```\n",
    "source /training/project_env.sh\n",
    "```\n",
    "#### Task 4: Launch Dataflow Pipeline\n",
    "- Return to the browser tab for Console. In the top search bar, enter Dataflow API. This will take you to the page, Navigation > APIs & Services > Dashboard > Google Dataflow API. It will either show a status information or it will give you the option to Enable the API.\n",
    "- If necessary, Enable the API.\n",
    "- Return to the second training_vm SSH terminal. Change into the directory for this lab.\n",
    "```\n",
    "cd ~/training-data-analyst/courses/streaming/process/sandiego\n",
    "cat run_oncloud.sh\n",
    "# github source code\n",
    "# https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/streaming/process/sandiego/run_oncloud.sh\n",
    "# check out java directory\n",
    "cd ~/training-data-analyst/courses/streaming/process/sandiego/src/main/java/com/google/cloud/training/dataanalyst/sandiego \n",
    "cat AverageSpeeds.java\n",
    "# github source code\n",
    "# https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/streaming/process/sandiego/src/main/java/com/google/cloud/training/dataanalyst/sandiego/AverageSpeeds.java\n",
    "# build a Dataflow streaming pipeline\n",
    "cd ~/training-data-analyst/courses/streaming/process/sandiego\n",
    "./run_oncloud.sh $DEVSHELL_PROJECT_ID $BUCKET AverageSpeeds\n",
    "```\n",
    "\n",
    "#### Task 5: Explore the pipeline\n",
    "- Return to the browser tab for Console. On the Navigation menu () click Dataflow and click on your job to monitor progress.\n",
    "- After the pipeline is running, click on the Navigation menu () click Pub/Sub>Topics.\n",
    "- Examine the line for Topic name for the topic sandiego. Notice that Subscriptions field is now at 1.\n",
    "- Return to the Navigation menu () click Dataflow and click on your job.\n",
    "- Compare the code in the Github browser tab, AverageSpeeds.java and the pipeline graph in the page for your Dataflow job.\n",
    "- Find the \"GetMessages\" pipeline step in the graph, and then find the corresponding code in the AverageSpeeds.java file. This is the pipeline step that reads from the Pub/Sub topic. It creates a collection of Strings - which corresponds to Pub/Sub messages that have been read.\n",
    "- Do you see a subscription created?\n",
    "- How does the code pull messages from Pub/Sub?\n",
    "- Find the \"Time Window\" pipeline step in the graph and in code. In this pipeline step we create a window of a duration specified in the pipeline parameters (sliding window in this case). This window will accumulate the traffic data from the previous step until end of window, and pass it to the next steps for further transforms.\n",
    "- What is the window interval?\n",
    "- How often is a new window created?\n",
    "- Find the \"BySensor\" and \"AvgBySensor\" pipeline steps in the graph, and then find the corresponding code snippet in the AverageSpeeds.java file. This \"BySensor\" does a grouping of all events in the window by sensor id, while \"AvgBySensor\" will then compute the mean speed for each grouping.\n",
    "- Find the \"ToBQRow\" pipeline step in the graph and in code. This step simply creates a \"row\" with the average computed from previous step together with the lane information.\n",
    "- In practice, other actions could be taken in the ToBQRow step. For example, it could compare the calculated mean against a predefined threshold and log the results of the comparison in Stackdriver Logging.\n",
    "\n",
    "- Find the \"BigQueryIO.Write\" in both the pipeline graph and in the source code. This step writes the row out of the pipeline into a BigQuery table. Because we chose the WriteDisposition.WRITE_APPEND write disposition, new records will be appended to the table.\n",
    "- Return to the BigQuery web UI tab Or open it from the Navigation menu () click BigQuery. Refresh your browser.\n",
    "- In the left column, beneath the text box, find your project name and the demos dataset you created. The small blue arrow to the left should now be active and clicking on it will reveal the average_speeds table. \n",
    "\n",
    "#### Task 6: Determine throughput rates\n",
    "- Return to the browser tab for Console. On the Navigation menu () click Dataflow and click on your job to monitor progress (it will have your username in the pipeline name).\n",
    "- Select the \"GetMessages\" pipeline node in the graph and look at the step metrics on the right.\n",
    "- System Lag is an important metric for streaming pipelines. It represents the amount of time data elements are waiting to be processed since they \"arrived\" in the input of the transformation step.\n",
    "- Elements Added metric under output collections tells you how many data elements exited this step (for the \"Read PubSub Msg\" step of the pipeline it also represents the number of Pub/Sub messages read from the topic by the Pub/Sub IO connector).\n",
    "- Select the \"Time Window\" node in the graph. Observe how the Elements Added metric under the Input Collections of the \"Time -Window\" step matches the Elements Added metric under the Output Collections of the previous step \"GetMessages\".\n",
    "\n",
    "#### Task 7: Review BigQuery output\n",
    "- Return to the BigQuery web UI or on the Navigation menu () click BigQuery.\n",
    "- Use the following query to observe the output from the Dataflow job. Replace <PROJECTID> with your Project ID. It is listed under connection details in Qwiklabs.\n",
    "\n",
    "```\n",
    "SELECT * \n",
    "FROM [<PROJECTID>:demos.average_speeds] \n",
    "ORDER BY timestamp DESC\n",
    "LIMIT 100\n",
    "```\n",
    "- Find the last update to the table by running the following SQL.\n",
    "\n",
    "```\n",
    "SELECT\n",
    "  MAX(timestamp)\n",
    "FROM\n",
    "  [<PROJECTID>:demos.average_speeds]\n",
    "```\n",
    "\n",
    "- Use the BigQuery Table Decorator to look at results in the last 10 minutes.\n",
    "```\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  [<PROJECTID>:demos.average_speeds@-600000]\n",
    "ORDER BY\n",
    "  timestamp DESC\n",
    "```\n",
    "\n",
    "#### Task 8: Observe and understand autoscaling\n",
    "- Return to the browser tab for Console. On the Navigation menu () click Dataflow and click on your pipeline job.\n",
    "- Examine the Job summary panel on the right, and review the Autoscaling section. How many workers are currently being used to process messages in the Pub/Sub topic?\n",
    "- Click on \"See more history\" and review how many workers were used at different points in time during pipeline execution.\n",
    "- The data from a traffic sensor simulator started at the beginning of the lab creates hundreds of messages per second in the Pub/Sub topic. This will cause Dataflow to increase the number of workers to keep the system lag of the pipeline at optimal levels.\n",
    "- Click on See more history. In the Worker History pop-up, you can see how Dataflow changed the number of workers. Notice the Rationale column that explains the reason for the change.\n",
    "\n",
    "#### Task 9: Refresh the sensor data simulation script\n",
    "- Return to the training_vm SSH terminal where the sensor data is script is running.\n",
    "- If you see messages that say \"INFO: Publishing\" then the script is still running. Press CRTL-C to stop it. Then issue the command to start the script again.\n",
    "\n",
    "```\n",
    "cd ~/training-data-analyst/courses/streaming/publish\n",
    "./send_sensor_data.py --speedFactor=60 --project (dollarsign)DEVSHELL_PROJECT_ID\n",
    "```\n",
    "\n",
    "- If the script has passed the quota limit, you will see repeating error messages that \"credentials could not be refreshed\" and you may not be able to use CTRL-C to stop the script. Simply close the SSH terminal. Open a new SSH terminal. The new session will have a fresh quota.\n",
    "- In the Console, on the Navigation menu () click Compute Engine > VM instances.\n",
    "- Locate the line with the instance called training_vm.\n",
    "- On the far right, under 'connect', Click on SSH to open a second terminal window.\n",
    "- In the training_vm SSH terminal, enter the following to create environment variables.\n",
    "\n",
    "```\n",
    "source /training/project_env.sh\n",
    "cd ~/training-data-analyst/courses/streaming/publish\n",
    "\n",
    "./send_sensor_data.py --speedFactor=60 --project (dollarsign)DEVSHELL_PROJECT_ID\n",
    "```\n",
    "\n",
    "#### Task 10: Stackdriver integration\n",
    "- Chart Dataflow metrics in Stackdriver Dashboards: Create Dashboards and chart time series of Dataflow metrics.\n",
    "- Configure Alerts: Define thresholds on job or resource group-level metrics and alert when these metrics reach specified values. Stackdriver alerting can notify on a variety of conditions such as long streaming system lag or failed jobs.\n",
    "- Monitor User-Defined Metrics: In addition to Dataflow metrics, Dataflow exposes user-defined metrics (SDK Aggregators) as Stackdriver custom counters in the Monitoring UI, available for charting and alerting. Any Aggregator defined in a Dataflow pipeline will be reported to Stackdriver as a custom metric. Dataflow will define a new custom metric on behalf of the user and report incremental updates to Stackdriver approximately every 30 seconds.\n",
    "\n",
    "#### Task 11: Explore metrics\n",
    "- Return to the browser tab for Console. On the Navigation menu () click Stackdriver > Monitoring.\n",
    "- Click Log in with Google.\n",
    "- Click Create Account.\n",
    "- Click Continue.\n",
    "- Click Skip AWS Setup.\n",
    "- Click Continue.\n",
    "- Select No Reports and click Continue.\n",
    "- It may take a few minutes for Stackdriver to import project information about your lab account and the resources already being used. Once the Launch monitoring button becomes active, click Launch monitoring.\n",
    "- The trial version of Stackdriver provides the Premium Tier of service. So upgrading simply sets up billing so the account will not revert to Basic Tier at the end of 30 days.\n",
    "- Click on Continue with the trial. (You can also click on 'Dismiss' on the message bar at the top asking if you want to upgrade).\n",
    "- Explore Stackdriver Metrics\n",
    "- In the panel to the left click on Resources > Metrics Explorer\n",
    "- In the Metrics Explorer, find and select the Dataflow_job resource type. You should see a list of available Dataflow-related metrics.\n",
    "\n",
    "\n",
    "- Select the resource Dataflow Job and the metric Data watermark lag.\n",
    "- Stackdriver will draw a graph on the right side of the page.\n",
    "- Under Find resource type and metric, click on the (x) to remove the Data watermark lag metric. Select a new metric, System Lag.\n",
    "- The metrics that Dataflow provides to Stackdriver are listed here:\n",
    "\n",
    "https://cloud.google.com/monitoring/api/metrics_gcp\n",
    "\n",
    "- Data watermark age: The age (time since event timestamp) of the most recent item of data that has been fully processed by the pipeline.\n",
    "- System lag: The current maximum duration that an item of data has been awaiting processing, in seconds.\n",
    "\n",
    "#### Task 12: Create alerts\n",
    "- On the Stackdriver Monitoring click on Stackdriver > Alerting > Policies Overview.\n",
    "- Click on Add Policy.\n",
    "- On the Create new Alerting Policy page click on Add Condition.\n",
    "- On the Metric Threshold row, click Select.\n",
    "- In the Target section, set the RESOURCE TYPE to Dataflow Job.\n",
    "- Under APPLIES TO, select Single.\n",
    "- Select the resource you used in the previous task.\n",
    "- In the Configuration section, set IF METRIC to System Lag.\n",
    "- Set CONDITION to above.\n",
    "- Set THRESHOLD to 5\n",
    "- Set FOR to 1 minute.\n",
    "- Click on Save Condition to save the alert.\n",
    "- Under Notification, click on the pulldown menu to view the options for notification channel. You can set up a notification policy if you would like, using your email address.\n",
    "- In the Name this policy section, give the policy a name such as MyAlertPolicy.\n",
    "- Click on Save Policy.\n",
    "- On the Stackdriver tab, click on Alerting > Events.\n",
    "- Every time an alert is triggered by a Metric Threshold condition, an Incident and a corresponding Event are created in Stackdriver. If you specified a notification mechanism in the alert (email, SMS, pager, etc), you will also receive a notification.\n",
    "\n",
    "#### Task 13: Set up dashboards\n",
    "- On the Stackdriver tab, click on Dashboards > Create dashboard.\n",
    "- Click on Add Chart.\n",
    "- On the Add Chart page:\n",
    "- In the Find resource type and metric box, start typing Dataflow Job and then select it as the Resource Type.\n",
    "- After you select a Resource Type, the Metric field menu will appear. Select a metric to chart, such as System Lag.\n",
    "in the Filter panel, select project, then the equals sign '=', then your Project ID.\n",
    "\n",
    "#### Task 14: Launch another streaming pipeline\n",
    "- In the training_vm SSH terminal, examine the CurrentConditions.java application. Do not make any changes to the code.\n",
    "```\n",
    "cd ~/training-data-analyst/courses/streaming/process/sandiego/src/main/java/com/google/cloud/training/dataanalyst/sandiego \n",
    "cat CurrentConditions.java\n",
    "### basic pipeline\n",
    "cd ~/training-data-analyst/courses/streaming/process/sandiego\n",
    "./run_oncloud.sh (dollarsign)DEVSHELL_PROJECT_ID (dollarsign)BUCKET CurrentConditions\n",
    "```\n",
    "\n",
    "- Return to the browser tab for Console. On the Navigation menu () click Dataflow and click on the new pipeline job. Confirm that the pipeline job is listed and verify that it is running without errors.\n",
    "- It will take several minutes before the current_conditions table appears in BigQuery.\n",
    "\n",
    "## End Lab\n",
    "\n",
    "## Module 3 Review\n",
    "\n",
    "1.) The Dataflow models provides constructs that map to the four questions that are relevant in any out-of-order data processing pipeline:\n",
    "\n",
    "- What results are calculated: Answered via transformations\n",
    "- Where in event time are results calculated: Answered via Event-time windowing \n",
    "- When in processing time are results materialized: Answered via Watermarks, triggers, and allowed lateness.\n",
    "- How do refinements of results relate: Answered via Accumulation modes\n",
    "\n",
    "\n",
    "## Lab: Streaming Analytics & Dashboards\n",
    "\n",
    "#### Objectives:\n",
    "- Connect to a BigQuery data source\n",
    "- Create reports and charts to visualize BigQuery data\n",
    "\n",
    "#### Task 1: Preparation\n",
    "-In the Console, on the Navigation menu () click Compute Engine > VM instances.\n",
    "-Locate the line with the instance called training_vm.\n",
    "-On the far right, under 'connect', Click on SSH to open a terminal window.\n",
    "-In this lab you will enter CLI commands on the training_vm.\n",
    "```\n",
    "ls /training\n",
    "cp -r /training/training-data-analyst/ .\n",
    "# set environment variables\n",
    "source /training/project_env.sh\n",
    "```\n",
    "\n",
    "#### Task 2.) Creating a Data Source\n",
    "- Google Data Studio is a separate service. Open a new browser tab. Navigate to: datastudio.google.com or click on this link: [Google Data Studio](https://datastudio.google.com/)\n",
    "    - The first step in creating a report in Data Studio is to create a data source for the report. A report may contain one or more data sources. When you create a BigQuery data source, Data Studio uses the BigQuery connector.\n",
    "    - You must have the appropriate permissions in order to add a BigQuery data source to a Data Studio report. In addition, the permissions applied to BigQuery datasets will apply to the reports, charts, and dashboards you create in Data Studio. When a Data Studio report is shared, the report components are visible only to users who have appropriate permissions.\n",
    "- On the Reports page, in the Start a new report section, click the Blank template. This starts the account setup process.\n",
    "- On the Welcome page, click on GET STARTED.\n",
    "- On the Terms page, click on the checkbox to acknowledge the terms. And click ACCEPT.\n",
    "- On the Preferences page, select No, thanks for each option to receive email notifications, and click DONE.\n",
    "- Now that the account is initialized, you need to start the process again.\n",
    "- On the Reports page, in the Start a new report section, click the Blank template. This time it will take you to a new page and begin an Untitled Report.\n",
    "- In the Add a data source panel on the right side, click CREATE NEW DATA SOURCE.\n",
    "- In the Google Connectors column on the left, select BigQuery.\n",
    "- Click on Authorize.\n",
    "- In the Sign in dialog, select your Qwiklabs student account.\n",
    "- Click ALLOW. to give Google Data Studio permission to view the BigQuery resources in your lab account.\n",
    "- Select My Projects.\n",
    "- In the Project column, click on your project name.\n",
    "- In the Dataset column, click on demos.\n",
    "- In the Table column, click current_conditions.\n",
    "- In the upper right corner of the window, click CONNECT.\n",
    "    - Once Data Studio has connected to the BigQuery data source, the table's fields are displayed. You can use this page to adjust the field properties or to create new calculated fields.\n",
    "- In the upper right corner, click ADD TO REPORT.\n",
    "- A verification panel opens. Click ADD TO REPORT.\n",
    "- This will initiate another sign in process to allow Data Studio to access Google Drive.\n",
    "- In the Sign in dialog, select your Qwiklabs student account.\n",
    "- Click ALLOW to give Google Data Studio permission to use the Google Drive resources in your lab account.\n",
    "\n",
    "#### Task 3: Creating a bar chart using a calculated field\n",
    "- Once you have added the current_conditions data source to the report, the next step is to create a visualization. Begin by creating a bar chart. The bar chart displays the total number of vehicles captured for each highway. To display this, you create a calculated field as follows.\n",
    "    - (Optional) At the top of the page, click Untitled Report to change the report name. For example, type <PROJECTID>-report1-yourname.\n",
    "- When the report editor loads, click Insert > Bar chart.\n",
    "- Using the handle, draw a rectangle on the report to display the chart.\n",
    "- In the Bar chart properties window, on the Data tab, notice the value for Data Source (current_conditions) and the default values for Dimension and Metric.\n",
    "- If Dimension is not set to highway, then change Dimension to highway. In the Dimension section, click the existing dimension.\n",
    "- In the Dimension picker, select highway.\n",
    "- Click the back arrow to close the Dimension picker.\n",
    "- In the Metric section, click +Add Metric here and add latitude.\n",
    "- Click the back arrow.\n",
    "- In the Metric section, mouse over Record Count and click the (x) to remove it.\n",
    "- In the Metric section, click the existing metric.\n",
    "- In the Metric picker, click CREATE NEW METRIC.\n",
    "- Click the button   Create a calculated field. To display a count of the number of vehicles using each highway, create a calculated field. For this lab, you count the entries in the sensorId field. The value is irrespective, we just need the number of occurrences.\n",
    "- For Field Name, type vehicles.\n",
    "- Leave the Field ID unchanged.\n",
    "- For Formula, type the following (or use the formula assistant): COUNT(sensorId).\n",
    "- Click SAVE.\n",
    "- Click DONE.\n",
    "- In the Metric picker, In the Metric section, click Add metric here.\n",
    "- Select vehicles. Click the back arrow. (this will display an error)\n",
    "- Click on the pencil next to Data Source, current_condition.\n",
    "- Examine the type associated with vehicles. If it is incorrectly set to timestamp, set it to numeric. Click Done. The error is corrected.\n",
    "- In the Metric section, mouse over latitude and click the (x) to remove it.\n",
    "- The Dimension should be set to highway and the Metric should be set to vehicles. Notice the chart is sorted in Descending order by default. The highway with the most vehicles are displayed first.\n",
    "- To enhance the chart, change the bar labels. In the Bar chart properties window, click the STYLE tab.\n",
    "- In the Bar chart section, check Show data labels.\n",
    "- The total number of vehicles is displayed above each bar in the chart.\n",
    "    \n",
    "#### Task 4: Creating a chart using a custom query\n",
    "- Because Data Studio does not allow aggregations on metrics, some report components are easier to generate using a custom SQL query. The Custom Query option also lets you leverage BigQuery's full query capabilities such as joins, unions, and analytical functions.\n",
    "- Alternatively, you can leverage BigQuery's full query capabilities by creating a view. A view is a virtual table defined by a SQL query. You can query data in a view by adding the dataset containing the view as a data source.\n",
    "- When you specify a SQL query as your BigQuery data source, the results of the query are in table format, which becomes the field definition (schema) for your data source. When you use a custom query as a data source, Data Studio uses your SQL as an inner select statement for each generated query to BigQuery. For more information on custom queries in Data Studio, consult the online help.\n",
    "\n",
    "- To add a bar chart to your report that uses a custom query data source:\n",
    "- Click Insert > Bar chart.\n",
    "- Using the handle, draw a rectangle on the report to display the chart.\n",
    "- In the Bar chart properties window, on the Data tab, notice the value for Data Source (natality) and the default values for Dimension and Metric are the same as the previous chart. In the Data Source section, click(Select data source).\n",
    "- Click Create new data source.\n",
    "- For Google Connectors, click BigQuery.\n",
    "- For My Projects, click Custom query.\n",
    "- For Project, select your project.\n",
    "- Type the following in the Enter custom query window\n",
    "- At the top of the window, click Untitled data source, and change the data source name to San Diego highway traffic summary.\n",
    "- In the upper right corner of the window, click Connect. Once Data Studio has connected to the BigQuery data source, the results of the query are used to determine the table schema.\n",
    "- When the schema is displayed, notice the type and aggregation for each field.\n",
    "- Click Add to report.\n",
    "- When prompted, click Add to report.\n",
    "- In the Bar chart properties, on the Data tab, in the Dimension section, click Invalid metric.\n",
    "- In the Metric picker, select maxspeed.\n",
    "- Click the back arrow to close the Metric picker.\n",
    "- In the Metric section, click +Add metric here.\n",
    "- In the Metric picker, select minspeed.\n",
    "- Click the back arrow to close the Metric picker.\n",
    "- In the Metric section, click Add a metric.\n",
    "- In the Metric picker, select avgspeed.\n",
    "- Click the back arrow to close the Metric picker.\n",
    "- For readability, change the chart styles. In the Bar chart properties, click the Style tab.\n",
    "- In the Color by section, click on the boxes to select different colors.\n",
    "- For readability, change the chart styles. In the Bar chart properties, click the Style tab.\n",
    "- In the Bar chart section, select different colors.\n",
    "\n",
    "#### Task 5: Viewing your query history\n",
    "- On the Navigation menu () click BigQuery.\n",
    "- Refresh the browser window.\n",
    "- Click Query History.\n",
    "- The list of queries is displayed with the most recent queries first. Click Open Query to view details on the query such as Job ID and Bytes Processed.\n",
    "\n",
    "## End Lab\n",
    "\n",
    "## Module 4 Review\n",
    "\n",
    "1.) Which of the following is true for Data Studio?\n",
    "- Data Studio supports data ingest through multiple connectors\n",
    "\n",
    "2.) Data Studio can issue queries to BigQuery\n",
    "- True\n",
    "\n",
    "### BigQuery vs. Big Table\n",
    "#### BigQuery\n",
    "- easy, inexpensive\n",
    "- latency in the order of seconds\n",
    "- 100k rows / second streaming\n",
    "\n",
    "#### Bigtable\n",
    "- low latency/high thoroughput\n",
    "- 100,000 QPS @ 6ms latency for a 10-node cluster\n",
    "\n",
    "## Lab: Streaming Data into Big Table\n",
    "#### Objectives:\n",
    "- Launch Dataflow pipeline to read from Pub/Sub and write into Bigtable\n",
    "- Open an HBase shell to query the Bigtable database\n",
    "\n",
    "#### Task 1: Preparation\n",
    "- In the Console, on the Navigation menu () click Compute Engine > VM instances.\n",
    "- Locate the line with the instance called training_vm.\n",
    "- On the far right, under 'connect', Click on SSH to open a terminal window.\n",
    "- In this lab you will enter CLI commands on the training_vm.\n",
    "```\n",
    "ls /training\n",
    "cp -r /training/training-data-analyst/ .\n",
    "# set environment variables\n",
    "source /training/project_env.sh\n",
    "# install files\n",
    "cd ~/training-data-analyst/courses/streaming/process/sandiego\n",
    "./install_quickstart.sh\n",
    "```\n",
    "\n",
    "#### Task 2: Simulate traffic sensor data in Pub / Sub\n",
    "```\n",
    "/training/sensor_magic.sh\n",
    "# start new terminal session for next line\n",
    "source /training/project_env.sh\n",
    "cd ~/training-data-analyst/courses/streaming/process/sandiego \n",
    "nano run_oncloud.sh\n",
    "cd ~/training-data-analyst/courses/streaming/process/sandiego\n",
    "./create_cbt.sh\n",
    "cd ~/training-data-analyst/courses/streaming/process/sandiego\n",
    "./run_oncloud.sh $DEVSHELL_PROJECT_ID $BUCKET CurrentConditions --bigtable\n",
    "cd ~/training-data-analyst/courses/streaming/process/sandiego/quickstart\n",
    "./quickstart.sh\n",
    "# hbase(main):001:0>\n",
    "# indicates it ran successfully\n",
    "scan 'current_conditions', {'LIMIT' => 2}\n",
    "scan 'current_conditions', {'LIMIT' => 10, STARTROW => '15#S#1', ENDROW => '15#S#999', COLUMN => 'lane:speed'}\n",
    "quit\n",
    "# cleanup\n",
    "cd ~/training-data-analyst/courses/streaming/process/sandiego\n",
    "./delete_cbt.sh\n",
    "```\n",
    "## End Lab\n",
    "\n",
    "## Module 5 Review\n",
    "\n",
    "1.) Which of the following are true about Cloud Bigtable?\n",
    "- Great for time series data\n",
    "- Offers low latency in the order of milliseconds\n",
    "- Support for SQL\n",
    "- Great for >1 TB of data\n",
    "\n",
    "Not sure ... \n",
    "\n",
    "2.) Cloud Big Table learns access patterns and attempts to distribute reads and storage across nodes evenly.\n",
    "- True\n",
    "\n",
    "3.) Which of the following can improve performance of Big Table\n",
    "- Change schema to minimize data skew\n",
    "- Clients and Big Table are in the same zone\n",
    "- Add more nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
